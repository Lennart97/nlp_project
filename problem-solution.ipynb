{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sifr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "#nltk stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90413/2685074158.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_slim = democrats.append(republican)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ExtractedTweets.csv\")\n",
    "df.head()\n",
    "\n",
    "democrats = df.query(\"Party == 'Democrat'\").sample(1000)\n",
    "republican = df.query(\"Party == 'Republican'\").sample(1000)\n",
    "\n",
    "df_slim = democrats.append(republican)\n",
    "df_slim.head()\n",
    "df = df_slim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract tweet information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_retweeted(tweet):\n",
    "    '''This function will extract the twitter handles of retweed people'''\n",
    "    return re.findall('(?<=RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.findall('(?<!RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "# make new columns for retweeted usernames, mentioned usernames and hashtags\n",
    "df['retweeted'] = df['Tweet'].apply(find_retweeted)\n",
    "df['mentioned'] = df['Tweet'].apply(find_mentioned)\n",
    "df['hashtags'] = df['Tweet'].apply(find_hashtags)\n",
    "df.head(30)\n",
    "\n",
    "print(\"Extracted tweet metadata\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean metioned, retweets and hashtags from tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"rt\")\n",
    "\n",
    "def clean_retweeted(tweet):\n",
    "    '''This function will extract the twitter handles of retweed people'''\n",
    "    return re.sub('(?<=RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean_mentioned(tweet):\n",
    "    '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.sub('(?<!RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean(tweet):\n",
    "    # Remove mentions, retweets and hashtags\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = clean_mentioned(tweet)\n",
    "    tweet = clean_retweeted(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(\"[\\\\.,;:!/\\\\?]*\", \"\", tweet)\n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    # Remove stop words\n",
    "    tweet = ' '.join([word for word in tweet.split(' ') if not word in stop_words])\n",
    "    # Remove multiple spaces\n",
    "    tweet = re.sub(\"\\\\s+\", \" \", tweet)\n",
    "    return tweet\n",
    "\n",
    "df['TweetCleaned'] = df['Tweet'].apply(clean)\n",
    "df.head()\n",
    "\n",
    "print(\"Cleaned tweets\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatize the text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore DeprecationWarning from SelectableGroups\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def lemmatization(tweet, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    doc = nlp(tweet)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            new_text.append(token.lemma_)\n",
    "    final = \" \".join(new_text)\n",
    "    return final\n",
    "\n",
    "df['TweetCleaned'] = df['TweetCleaned'].apply(lemmatization)\n",
    "df.head()\n",
    "print(\"Lemmatized tweets\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gensim.utils.simple_preprocess(\"today vote proud support similar legislation\", deacc=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_words(tweet):\n",
    "    # Use gensims simple simple_preprocess to remove accents, lowercase and tokenize\n",
    "    return gensim.utils.simple_preprocess(tweet, deacc=True)\n",
    "\n",
    "df['TweetWords'] = df['TweetCleaned'].apply(gen_words)\n",
    "df.head()\n",
    "\n",
    "print(\"Generated tokens\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#BIGRAMS AND TRIGRAMS\n",
    "bigram_phrases = gensim.models.Phrases(df['TweetWords'], min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[df['TweetWords']], threshold=100)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(tweetWords):\n",
    "    return bigram[tweetWords]\n",
    "\n",
    "def make_trigrams(bigrams):\n",
    "    return trigram[bigram[bigrams]]\n",
    "\n",
    "df['bigramms'] =  df['TweetWords'].apply(make_bigrams)\n",
    "df['trigrams'] = df['bigramms'].apply(make_trigrams)\n",
    "df.head(30)\n",
    "\n",
    "print(\"Generated trigrams\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(df['trigrams'])\n",
    "\n",
    "texts = df['trigrams']\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# print (corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow\n",
    "\n",
    "print(\"TF-IDF removal\")\n",
    "print(\"Finish preprocessing\")\n",
    "\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"data.bin\", \"wb\")\n",
    "pickle.dump((df, id2word, corpus, tfidf), file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "(df, id2word, corpus, tfidf) = pickle.load(open(\"data.bin\", \"rb\"))\n",
    "df.head()\n",
    "texts = df['trigrams']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "                corpus=corpus[:-1],\n",
    "                id2word=id2word,\n",
    "                num_topics=10,\n",
    "                random_state=100,\n",
    "                chunksize=100,\n",
    "                passes=10\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_document_topic(model, tri):\n",
    "    doc_bow = id2word.doc2bow(tri)\n",
    "    vec = model.get_document_topics(bow=doc_bow, minimum_probability=0)\n",
    "    return [v for _, v in vec]\n",
    "\n",
    "#df['topic_vec'] = df['trigrams'].apply(lambda trigram: get_document_topic(lda_model, trigram))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def evaluate_model(model):\n",
    "    df['topic_vec'] = df['trigrams'].apply(lambda trigram: get_document_topic(model, trigram))\n",
    "    df['len'] = df['topic_vec'].apply(lambda x: len(x))\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    text_train, text_test, label_train, label_test = train_test_split(df[\"topic_vec\"], df['Party'], train_size=0.70, random_state=101, test_size=0.30, shuffle=True)\n",
    "    classifiers = [('MLP', MLPClassifier()), ('G-NB', GaussianNB()), ('L-SVM', LinearSVC())]\n",
    "    for name, cl in classifiers:\n",
    "\n",
    "        # Train and predict\n",
    "        cl.fit(text_train.to_list(), label_train)\n",
    "\n",
    "        prediction = cl.predict(text_test.to_list())\n",
    "\n",
    "        # Confusion matrix\n",
    "        confusion = confusion_matrix(label_test, prediction)\n",
    "        # increase size of plot and fontsize for improved visibility\n",
    "        #fig, ax_cm = plt.subplots(figsize=(10, 10))\n",
    "        #plt.rcParams.update({'font.size': 14})\n",
    "        # create plot for multi-class confusion matrix, use prepared axes object and change colormap for better contrast\n",
    "        #plot_confusion_matrix(cl, text_test.to_list(), label_test.to_list(), ax=ax_cm, cmap=plt.cm.Reds_r)\n",
    "        # show plot corresponding to printed name and accuracy\n",
    "        #plt.show()\n",
    "        accuracy = accuracy_score(label_test, prediction)\n",
    "        precision, recall, fscore, *rest = precision_recall_fscore_support(label_test, prediction, average='macro')\n",
    "        result[name] = confusion, accuracy, precision, recall, fscore\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the model see: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#5preparestopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}