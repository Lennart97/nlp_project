{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sifr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "#nltk stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90413/2685074158.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_slim = democrats.append(republican)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ExtractedTweets.csv\")\n",
    "df.head()\n",
    "\n",
    "democrats = df.query(\"Party == 'Democrat'\").sample(1000)\n",
    "republican = df.query(\"Party == 'Republican'\").sample(1000)\n",
    "\n",
    "df_slim = democrats.append(republican)\n",
    "df_slim.head()\n",
    "df = df_slim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract tweet information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tweet metadata\n"
     ]
    }
   ],
   "source": [
    "def find_retweeted(tweet):\n",
    "    '''This function will extract the twitter handles of retweed people'''\n",
    "    return re.findall('(?<=RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.findall('(?<!RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "# make new columns for retweeted usernames, mentioned usernames and hashtags\n",
    "df['retweeted'] = df['Tweet'].apply(find_retweeted)\n",
    "df['mentioned'] = df['Tweet'].apply(find_mentioned)\n",
    "df['hashtags'] = df['Tweet'].apply(find_hashtags)\n",
    "df.head(30)\n",
    "\n",
    "print(\"Extracted tweet metadata\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean metioned, retweets and hashtags from tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned tweets\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"rt\")\n",
    "\n",
    "def clean_retweeted(tweet):\n",
    "    '''This function will extract the twitter handles of retweed people'''\n",
    "    return re.sub('(?<=RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean_mentioned(tweet):\n",
    "    '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.sub('(?<!RT\\\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "def clean(tweet):\n",
    "    # Remove mentions, retweets and hashtags\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = clean_mentioned(tweet)\n",
    "    tweet = clean_retweeted(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(\"[\\\\.,;:!/\\\\?]*\", \"\", tweet)\n",
    "    # Remove stop words\n",
    "    tweet = ' '.join([word for word in tweet.split(' ') if not word in stop_words])\n",
    "    # Remove multiple spaces\n",
    "    tweet = re.sub(\"\\\\s+\", \" \", tweet)\n",
    "    return tweet\n",
    "\n",
    "df['TweetCleaned'] = df['Tweet'].apply(clean)\n",
    "df.head()\n",
    "\n",
    "print(\"Cleaned tweets\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatize the text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized tweets\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore DeprecationWarning from SelectableGroups\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def lemmatization(tweet, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    doc = nlp(tweet)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            new_text.append(token.lemma_)\n",
    "    final = \" \".join(new_text)\n",
    "    return final\n",
    "\n",
    "df['TweetCleaned'] = df['TweetCleaned'].apply(lemmatization)\n",
    "df.head()\n",
    "print(\"Lemmatized tweets\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "          Party           Handle  \\\n3043   Democrat  CongressmanRaja   \n1058   Democrat      RepBarragan   \n32345  Democrat  BennieGThompson   \n25253  Democrat   RepAndreCarson   \n21433  Democrat    SanfordBishop   \n\n                                                   Tweet       retweeted  \\\n3043   Proud to have introduced the Help Students Vot...              []   \n1058   RT @GreenForAll: Thank you @RepMcEachin, @RepJ...  [@GreenForAll]   \n32345  RT @OfficialCBC: “I marvel at some of my colle...  [@OfficialCBC]   \n25253  My annual Youth Opportunities Fair is TODAY, M...              []   \n21433  With 755 farms and more than 423,000 acres ded...              []   \n\n                                       mentioned hashtags  \\\n3043                                [@SenBooker]       []   \n1058   [@RepMcEachin, @RepJayapal, @RepBarragan]       []   \n32345                                         []       []   \n25253                                         []       []   \n21433                                         []       []   \n\n                                            TweetCleaned  \n3043   proud introduce help student vote act exactly ...  \n1058   thank leadership fight environmental justice call  \n32345  marvel colleague probably hit gun want defend ...  \n25253  annual youth opportunity fair today march hope...  \n21433  farm acre dedicate cotton second congressional...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Party</th>\n      <th>Handle</th>\n      <th>Tweet</th>\n      <th>retweeted</th>\n      <th>mentioned</th>\n      <th>hashtags</th>\n      <th>TweetCleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3043</th>\n      <td>Democrat</td>\n      <td>CongressmanRaja</td>\n      <td>Proud to have introduced the Help Students Vot...</td>\n      <td>[]</td>\n      <td>[@SenBooker]</td>\n      <td>[]</td>\n      <td>proud introduce help student vote act exactly ...</td>\n    </tr>\n    <tr>\n      <th>1058</th>\n      <td>Democrat</td>\n      <td>RepBarragan</td>\n      <td>RT @GreenForAll: Thank you @RepMcEachin, @RepJ...</td>\n      <td>[@GreenForAll]</td>\n      <td>[@RepMcEachin, @RepJayapal, @RepBarragan]</td>\n      <td>[]</td>\n      <td>thank leadership fight environmental justice call</td>\n    </tr>\n    <tr>\n      <th>32345</th>\n      <td>Democrat</td>\n      <td>BennieGThompson</td>\n      <td>RT @OfficialCBC: “I marvel at some of my colle...</td>\n      <td>[@OfficialCBC]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>marvel colleague probably hit gun want defend ...</td>\n    </tr>\n    <tr>\n      <th>25253</th>\n      <td>Democrat</td>\n      <td>RepAndreCarson</td>\n      <td>My annual Youth Opportunities Fair is TODAY, M...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>annual youth opportunity fair today march hope...</td>\n    </tr>\n    <tr>\n      <th>21433</th>\n      <td>Democrat</td>\n      <td>SanfordBishop</td>\n      <td>With 755 farms and more than 423,000 acres ded...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>farm acre dedicate cotton second congressional...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(\"today vote proud support similar legislation\", deacc=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens\n"
     ]
    }
   ],
   "source": [
    "def gen_words(tweet):\n",
    "    # Use gensims simple simple_preprocess to remove accents, lowercase and tokenize\n",
    "    return gensim.utils.simple_preprocess(tweet, deacc=True)\n",
    "\n",
    "df['TweetWords'] = df['TweetCleaned'].apply(gen_words)\n",
    "df.head()\n",
    "\n",
    "print(\"Generated tokens\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated trigrams\n"
     ]
    }
   ],
   "source": [
    "#BIGRAMS AND TRIGRAMS\n",
    "bigram_phrases = gensim.models.Phrases(df['TweetWords'], min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[df['TweetWords']], threshold=100)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(tweetWords):\n",
    "    return bigram[tweetWords]\n",
    "\n",
    "def make_trigrams(bigrams):\n",
    "    return trigram[bigram[bigrams]]\n",
    "\n",
    "df['bigramms'] =  df['TweetWords'].apply(make_bigrams)\n",
    "df['trigrams'] = df['bigramms'].apply(make_trigrams)\n",
    "df.head(30)\n",
    "\n",
    "print(\"Generated trigrams\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF removal\n",
      "Finish preprocessing\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(df['trigrams'])\n",
    "\n",
    "texts = df['trigrams']\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# print (corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow\n",
    "\n",
    "print(\"TF-IDF removal\")\n",
    "print(\"Finish preprocessing\")\n",
    "\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"data.bin\", \"wb\")\n",
    "pickle.dump((df, id2word, corpus, tfidf), file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "          Party           Handle  \\\n3043   Democrat  CongressmanRaja   \n1058   Democrat      RepBarragan   \n32345  Democrat  BennieGThompson   \n25253  Democrat   RepAndreCarson   \n21433  Democrat    SanfordBishop   \n\n                                                   Tweet       retweeted  \\\n3043   Proud to have introduced the Help Students Vot...              []   \n1058   RT @GreenForAll: Thank you @RepMcEachin, @RepJ...  [@GreenForAll]   \n32345  RT @OfficialCBC: “I marvel at some of my colle...  [@OfficialCBC]   \n25253  My annual Youth Opportunities Fair is TODAY, M...              []   \n21433  With 755 farms and more than 423,000 acres ded...              []   \n\n                                       mentioned hashtags  \\\n3043                                [@SenBooker]       []   \n1058   [@RepMcEachin, @RepJayapal, @RepBarragan]       []   \n32345                                         []       []   \n25253                                         []       []   \n21433                                         []       []   \n\n                                            TweetCleaned  \\\n3043   proud introduce help student vote act exactly ...   \n1058   thank leadership fight environmental justice call   \n32345  marvel colleague probably hit gun want defend ...   \n25253  annual youth opportunity fair today march hope...   \n21433  farm acre dedicate cotton second congressional...   \n\n                                              TweetWords  \\\n3043   [proud, introduce, help, student, vote, act, e...   \n1058   [thank, leadership, fight, environmental, just...   \n32345  [marvel, colleague, probably, hit, gun, want, ...   \n25253  [annual, youth, opportunity, fair, today, marc...   \n21433  [farm, acre, dedicate, cotton, second, congres...   \n\n                                                bigramms  \\\n3043   [proud, introduce, help, student, vote, act, e...   \n1058   [thank, leadership, fight, environmental, just...   \n32345  [marvel, colleague, probably, hit, gun, want, ...   \n25253  [annual, youth, opportunity, fair, today, marc...   \n21433  [farm, acre, dedicate, cotton, second, congres...   \n\n                                                trigrams  \n3043   [proud, introduce, help, student, vote, act, e...  \n1058   [thank, leadership, fight, environmental, just...  \n32345  [marvel, colleague, probably, hit, gun, want, ...  \n25253  [annual, youth, opportunity, fair, today, marc...  \n21433  [farm, acre, dedicate, cotton, second, congres...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Party</th>\n      <th>Handle</th>\n      <th>Tweet</th>\n      <th>retweeted</th>\n      <th>mentioned</th>\n      <th>hashtags</th>\n      <th>TweetCleaned</th>\n      <th>TweetWords</th>\n      <th>bigramms</th>\n      <th>trigrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3043</th>\n      <td>Democrat</td>\n      <td>CongressmanRaja</td>\n      <td>Proud to have introduced the Help Students Vot...</td>\n      <td>[]</td>\n      <td>[@SenBooker]</td>\n      <td>[]</td>\n      <td>proud introduce help student vote act exactly ...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n    </tr>\n    <tr>\n      <th>1058</th>\n      <td>Democrat</td>\n      <td>RepBarragan</td>\n      <td>RT @GreenForAll: Thank you @RepMcEachin, @RepJ...</td>\n      <td>[@GreenForAll]</td>\n      <td>[@RepMcEachin, @RepJayapal, @RepBarragan]</td>\n      <td>[]</td>\n      <td>thank leadership fight environmental justice call</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n    </tr>\n    <tr>\n      <th>32345</th>\n      <td>Democrat</td>\n      <td>BennieGThompson</td>\n      <td>RT @OfficialCBC: “I marvel at some of my colle...</td>\n      <td>[@OfficialCBC]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>marvel colleague probably hit gun want defend ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n    </tr>\n    <tr>\n      <th>25253</th>\n      <td>Democrat</td>\n      <td>RepAndreCarson</td>\n      <td>My annual Youth Opportunities Fair is TODAY, M...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>annual youth opportunity fair today march hope...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n    </tr>\n    <tr>\n      <th>21433</th>\n      <td>Democrat</td>\n      <td>SanfordBishop</td>\n      <td>With 755 farms and more than 423,000 acres ded...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>farm acre dedicate cotton second congressional...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df, id2word, corpus, tfidf) = pickle.load(open(\"data.bin\", \"rb\"))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "                corpus=corpus[:-1],\n",
    "                id2word=id2word,\n",
    "                num_topics=10,\n",
    "                random_state=100,\n",
    "                chunksize=100,\n",
    "                passes=10\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/sifr/uni/nlp/nlp_project/venv/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "text/plain": "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\ntopic                                                \n1     -0.093347  0.177555       1        1  13.239076\n8      0.072870 -0.213652       2        1  12.581890\n6      0.029316  0.196284       3        1  12.014695\n3     -0.106697 -0.196148       4        1  10.594650\n9     -0.201831  0.090145       5        1   9.880935\n4      0.186275  0.150415       6        1   8.720211\n2      0.213423 -0.091163       7        1   8.648675\n0     -0.207863 -0.054541       8        1   8.523488\n5      0.136117  0.009657       9        1   8.263996\n7     -0.028264 -0.068553      10        1   7.532384, topic_info=         Term       Freq      Total Category  logprob  loglift\n214      bill  51.000000  51.000000  Default  30.0000  30.0000\n456      week  56.000000  56.000000  Default  29.0000  29.0000\n156  national  37.000000  37.000000  Default  28.0000  28.0000\n110      meet  43.000000  43.000000  Default  27.0000  27.0000\n0         act  35.000000  35.000000  Default  26.0000  26.0000\n..        ...        ...        ...      ...      ...      ...\n126      work   4.844592  73.739595  Topic10  -5.5344  -0.1367\n203       job   4.056423  37.773213  Topic10  -5.7119   0.3547\n167      year   4.066547  71.596714  Topic10  -5.7094  -0.2823\n696  business   3.798612  19.304731  Topic10  -5.7776   0.9602\n238     trump   3.723511  16.413190  Topic10  -5.7976   1.1025\n\n[633 rows x 6 columns], token_table=      Topic      Freq      Term\nterm                           \n3331      1  0.817179  abortion\n2390      7  0.848237   academy\n266       1  0.782259    access\n266       2  0.097782    access\n266       3  0.097782    access\n...     ...       ...       ...\n274       1  0.670475     young\n274       3  0.074497     young\n274       5  0.148994     young\n274       7  0.074497     young\n274       9  0.074497     young\n\n[1302 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 9, 7, 4, 10, 5, 3, 1, 6, 8])",
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el904131402245504593603175286789\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el904131402245504593603175286789_data = {\"mdsDat\": {\"x\": [-0.09334744555178712, 0.07287026364914877, 0.029316208716114164, -0.10669650264572626, -0.20183108930466642, 0.1862747172714042, 0.21342319864623335, -0.2078626701401166, 0.1361169485920715, -0.028263629232675436], \"y\": [0.1775547023348282, -0.21365244030299976, 0.1962844782243437, -0.19614812893337974, 0.09014501338693634, 0.150415411590575, -0.09116270414106249, -0.05454076668942009, 0.009657375861399677, -0.06855294133122071], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [13.23907587042314, 12.58188967401841, 12.014694544920896, 10.594650071825066, 9.880935241883911, 8.720211340114629, 8.648675296940882, 8.523488353380026, 8.263995579563044, 7.532384026929996]}, \"tinfo\": {\"Term\": [\"bill\", \"week\", \"national\", \"meet\", \"act\", \"day\", \"benefit\", \"here\", \"protect\", \"chairman\", \"step\", \"last\", \"happy\", \"state\", \"open\", \"thank\", \"enjoy\", \"hold\", \"world\", \"country\", \"lose\", \"discuss\", \"small_business\", \"anniversary\", \"release\", \"mark\", \"tax_reform\", \"stop\", \"watch\", \"tonight\", \"order\", \"commit\", \"pro\", \"clean\", \"reduce\", \"market\", \"bank\", \"abortion\", \"neighborhood\", \"speaker\", \"hostage\", \"hoosier\", \"regime\", \"large\", \"exactly\", \"peace\", \"httpstcov\", \"hour\", \"limited\", \"treatment\", \"keynote\", \"survivor\", \"directly\", \"gavel\", \"access\", \"threaten\", \"allow\", \"strategic\", \"final\", \"fed\", \"tour\", \"httpstcoi\", \"fall\", \"research\", \"real\", \"healthcare\", \"staff\", \"come\", \"meet\", \"young\", \"ve\", \"health_care\", \"thank\", \"today\", \"support\", \"time\", \"yesterday\", \"student\", \"legislation\", \"give\", \"discuss\", \"honor\", \"hear\", \"live\", \"help\", \"tomorrow\", \"people\", \"httpstco\", \"speak\", \"community\", \"continue\", \"military\", \"join\", \"pass\", \"vote\", \"need\", \"team\", \"impact\", \"story\", \"httpstcos\", \"highlight\", \"personal\", \"sell\", \"game\", \"parent\", \"response\", \"reauthorization\", \"leg\", \"hoosi\", \"old\", \"value\", \"army\", \"coffee\", \"practice\", \"au\", \"winner\", \"taxpayer\", \"luck\", \"racist\", \"attention\", \"understand\", \"resilient\", \"basketball\", \"genocide\", \"amp\", \"operate\", \"ceremony\", \"food\", \"protect\", \"deny\", \"spend\", \"stop\", \"public\", \"break\", \"bipartisan\", \"last\", \"week\", \"pay\", \"opioid\", \"state\", \"service\", \"federal\", \"job\", \"work\", \"child\", \"help\", \"important\", \"pass\", \"tax\", \"bill\", \"httpstco\", \"family\", \"thank\", \"great\", \"see\", \"year\", \"today\", \"people\", \"get\", \"education\", \"organization\", \"play\", \"stay\", \"mom\", \"bring\", \"unemployment\", \"elect_official\", \"executive\", \"reform\", \"disposal\", \"assassination\", \"privilege\", \"career\", \"more\", \"korean\", \"winter\", \"amazing\", \"signing\", \"educator\", \"wa\", \"blessing\", \"conversation\", \"specific\", \"plane\", \"existence\", \"particularly\", \"me\", \"recall\", \"heritage\", \"enjoy\", \"tonight\", \"serve\", \"officer\", \"incredible\", \"pm\", \"person\", \"result\", \"woman\", \"look_forward\", \"member\", \"safe\", \"work\", \"thank\", \"folk\", \"meet\", \"provide\", \"join\", \"great\", \"school\", \"leader\", \"many\", \"httpstco\", \"discuss\", \"support\", \"meeting\", \"well\", \"year\", \"today\", \"big\", \"bonus\", \"tax_code\", \"priority\", \"north\", \"less\", \"detail\", \"view\", \"authorization\", \"overdose\", \"mental\", \"agriculture\", \"train\", \"appearance\", \"corner\", \"reminder\", \"walk\", \"nearly\", \"trust\", \"farmer\", \"finally\", \"corruption\", \"short\", \"current\", \"senseless\", \"effect\", \"reg\", \"crucial\", \"idea\", \"worth\", \"wrap\", \"sure\", \"counsel\", \"employee\", \"open\", \"corporation\", \"day\", \"control\", \"heart\", \"congressman\", \"rate\", \"ever\", \"visit\", \"year\", \"last\", \"go\", \"today\", \"plan\", \"look\", \"way\", \"tomorrow\", \"need\", \"take\", \"first\", \"house\", \"tax\", \"time\", \"thank\", \"httpstco\", \"family\", \"work\", \"office\", \"new\", \"anniversary\", \"victim\", \"king\", \"examine\", \"commercial\", \"package\", \"block\", \"restore\", \"strike\", \"lose\", \"uniform\", \"motorcycle\", \"crop\", \"extra\", \"branch\", \"couple\", \"man_woman\", \"room\", \"reopen\", \"nature\", \"invite\", \"aluminum\", \"steel\", \"coach\", \"identify\", \"hot\", \"criminal\", \"rapist\", \"loss\", \"announcement\", \"immigration\", \"httpstcoc\", \"voting\", \"month\", \"travel\", \"httpstcow\", \"director\", \"start\", \"former\", \"remember\", \"vote\", \"want\", \"president\", \"th\", \"honor\", \"funding\", \"life\", \"family\", \"get\", \"httpstco\", \"military\", \"service\", \"new\", \"hearing\", \"thank\", \"first\", \"join\", \"today\", \"year\", \"continue\", \"say\", \"give\", \"go\", \"mark\", \"chairman\", \"claim\", \"perfect\", \"nomination\", \"approve\", \"asset\", \"test\", \"consider\", \"appropriation\", \"bc\", \"release\", \"forest\", \"premium\", \"orange\", \"secret\", \"sponsor\", \"prisoner\", \"self\", \"pioneer\", \"music\", \"brick\", \"worship\", \"group\", \"authorize\", \"excellence\", \"pen\", \"strengthen\", \"award\", \"redeem\", \"bill\", \"improve\", \"congrat\", \"subcommittee\", \"push\", \"committee\", \"hurricane\", \"low\", \"update\", \"opportunity\", \"read\", \"receive\", \"act\", \"hearing\", \"watch\", \"goal\", \"proud\", \"make\", \"re\", \"today\", \"pass\", \"statement\", \"go\", \"use\", \"httpstco\", \"program\", \"honor\", \"tax\", \"tax_reform\", \"law_enforcement\", \"here\", \"currently\", \"nd\", \"invest\", \"vet\", \"rd\", \"fix\", \"httpst\", \"icymi\", \"academy\", \"line\", \"entrance\", \"skill\", \"readiness\", \"cast\", \"reason\", \"un\", \"assist\", \"evening\", \"harvey\", \"complete\", \"monument\", \"stagnant\", \"immoral\", \"generating\", \"unique\", \"council\", \"regard\", \"resident\", \"small_business\", \"acre\", \"benefit\", \"power\", \"week\", \"money\", \"economy\", \"fund\", \"state\", \"new\", \"fully\", \"good\", \"happy_birthday\", \"colleague\", \"country\", \"people\", \"httpstco\", \"national\", \"today\", \"join\", \"support\", \"great\", \"meet\", \"local\", \"weekly\", \"human\", \"operation\", \"turn\", \"trafficking\", \"sit\", \"air\", \"complain\", \"participate\", \"newsletter\", \"firefighter\", \"hall\", \"produce\", \"historic\", \"sector\", \"accord\", \"rural\", \"development\", \"pill\", \"accountable\", \"deep\", \"aggie\", \"recognize\", \"ca\", \"pipeline\", \"responsibility\", \"material\", \"httpstcovg\", \"process\", \"httptco\", \"online\", \"importance\", \"hold\", \"veteran\", \"build\", \"last_night\", \"social\", \"strong\", \"full\", \"cut\", \"company\", \"summit\", \"discuss\", \"tax_cut\", \"say\", \"st\", \"today\", \"election\", \"time\", \"continue\", \"httpstco\", \"great\", \"student\", \"family\", \"job\", \"thank\", \"blue\", \"sex\", \"wish\", \"quality\", \"already\", \"paycheck\", \"extend\", \"park\", \"wear\", \"percent\", \"treasure\", \"whole\", \"date\", \"flight\", \"outdated\", \"cap\", \"counting\", \"nuclear\", \"department\", \"shift\", \"employ\", \"free\", \"robotic\", \"space\", \"main\", \"meaningful\", \"indy\", \"critic\", \"article\", \"world\", \"national\", \"hard\", \"cosponsor\", \"rank\", \"happy\", \"celebrate\", \"love\", \"budget\", \"grow\", \"country\", \"see\", \"httpstco\", \"big\", \"hear\", \"make\", \"much\", \"give\", \"congratulation\", \"pre\", \"today\", \"proud\", \"family\", \"join\", \"support\", \"need\", \"work\", \"innovation\", \"everyday\", \"accountability\", \"responder\", \"found\", \"se\", \"storm\", \"dyess\", \"brief\", \"necessary\", \"fun\", \"saving\", \"forecasting\", \"withhold\", \"entire\", \"min\", \"festival\", \"payment\", \"candidate\", \"rule\", \"graham\", \"rm\", \"stable\", \"evidence\", \"enforcement\", \"institution\", \"jake\", \"laird\", \"exchange\", \"subsidy\", \"crime\", \"constituent\", \"victory\", \"step\", \"democracy\", \"experience\", \"check\", \"httpstcob\", \"gun\", \"affect\", \"urge\", \"campaign\", \"book\", \"class\", \"effort\", \"senator\", \"act\", \"fact\", \"question\", \"right\", \"great\", \"help\", \"well\", \"take\", \"people\", \"community\", \"plan\", \"hear\", \"say\", \"work\", \"job\", \"year\", \"business\", \"trump\"], \"Freq\": [51.0, 56.0, 37.0, 43.0, 35.0, 48.0, 19.0, 15.0, 26.0, 13.0, 17.0, 38.0, 31.0, 35.0, 23.0, 110.0, 21.0, 19.0, 16.0, 32.0, 15.0, 45.0, 15.0, 12.0, 13.0, 11.0, 11.0, 28.0, 25.0, 16.0, 7.13135933396448, 5.884982397138559, 5.486044415648805, 5.520516644902209, 5.054211211490048, 4.693233852042114, 4.3940052623497525, 4.202754680003491, 3.9612668232748307, 3.8955926074892884, 3.763126062685647, 3.905654622439295, 3.5498154627318117, 6.661626258415515, 3.371415902523187, 2.9563968218946055, 2.8487996357819556, 8.72598862804743, 2.6347986875143508, 2.590125118469434, 2.5898349211419003, 2.5907474360948286, 2.574869675913875, 2.5370963265418998, 8.052577696361347, 4.717921711885522, 5.465147195637369, 2.457004374500089, 5.2567358093895455, 2.1860983912820706, 9.018742503663884, 6.023346273792047, 6.185450601366701, 7.01207819091809, 5.8815221203541945, 5.92023032686089, 9.346736777930152, 13.371379082623548, 22.972430137916454, 8.562691880658084, 6.443023824841947, 7.868243148940307, 34.91813702794781, 39.009160214064295, 20.864472899859354, 18.80315007109403, 10.653469236096583, 15.357808909739125, 10.720598204056053, 11.595183663375648, 14.787277951378766, 13.803976941678211, 12.737277839102266, 10.950915471960666, 12.813405936644285, 8.75956114517118, 11.918929408601104, 17.923950574680948, 9.949641306670056, 9.95994682629801, 9.82250073634821, 8.408293848551105, 10.203399288809784, 9.213473947726047, 9.136901014834208, 8.578053260300338, 8.693921828689561, 10.652914150342951, 12.659499194884857, 5.4291270759215315, 5.263229142710345, 5.14898589952304, 4.209868449941539, 4.030755405514436, 3.8265631852834563, 3.7557237762872195, 3.6244732823552397, 3.540456695467092, 3.341867632768193, 3.2595661569887118, 3.234757763088902, 3.2341250638734085, 3.729571731564763, 3.198510444119235, 3.1984331460250344, 5.756226844546857, 5.592523795491982, 2.979870160354967, 3.2011063717828034, 2.737831934335685, 3.4921878528286863, 2.6133082831940717, 2.5873497222814814, 2.57513638482343, 2.536195084126255, 2.514674244973617, 4.651799786137808, 8.338308606278826, 18.22275277870623, 3.9804760967037214, 7.921348848847901, 16.913899458614317, 11.573924759932337, 8.313288835491788, 12.65461796338071, 16.482305072496818, 21.616562822182274, 10.511711526782651, 7.275113878192284, 13.890814892113946, 12.045999489974442, 8.599823327718633, 13.802930776197758, 20.428036868729215, 9.642977332011858, 15.315372961952031, 8.953759849493439, 12.693896846804101, 10.305676796732193, 14.11167654178902, 22.09842768985899, 13.406945999772772, 15.671777296956499, 13.057266598963752, 10.483390841084013, 11.680813709749808, 10.181817575196593, 9.042998113504325, 8.877357840607141, 8.652475607586247, 6.630536366351782, 7.562035985598837, 5.425113121238006, 4.595097916447201, 13.569415308276321, 4.1828095659957425, 7.281967160173137, 3.703717970883504, 8.405027325797084, 3.1974864826051927, 3.1206819678087556, 3.0858736748074627, 3.5116815098525476, 3.0141360905141488, 2.7386544080082986, 2.7160973291941364, 5.214048958999757, 2.5554224819871987, 4.6678060112365936, 2.4508433692730276, 2.367437104909596, 4.737881881079578, 2.2574019462164086, 2.254214525476819, 2.252601108731928, 4.079625619074528, 2.176472761811633, 2.1757494353268787, 2.1579412511819136, 15.737466499431127, 12.146631429306773, 13.418810304833988, 5.481522107358028, 7.4540742109490346, 7.973306894764968, 3.210074413814203, 4.851114028925696, 12.694711968070258, 6.645929209586082, 15.68893550696238, 8.428419818223718, 22.006749242624593, 27.036377514155234, 5.054892072223659, 13.199522319147974, 8.421442166704017, 14.087511173459523, 14.377896115383692, 9.078157002883604, 7.508760427028833, 9.207798068333156, 15.249278760521477, 10.078597495895833, 10.166954047247366, 7.435515134176117, 7.515788654447626, 9.361013881701563, 10.338698865232473, 7.089668479052746, 11.196698952393515, 8.567851835772359, 7.1533884502361405, 6.964476028574829, 5.2143347218011575, 5.119236457361868, 4.744518623147861, 3.181143800411149, 3.1617399600010283, 3.036586314355815, 2.8732220414963865, 2.8693685144873697, 2.8672914831170107, 2.8178732436461984, 5.0623275254927025, 2.7099411391775035, 7.762207949801497, 5.019769576597999, 2.460441437219589, 2.3741469009690257, 2.349484689718481, 2.1928187877829215, 4.0489073774817, 2.1206297434211665, 2.120542756808983, 2.11991536837977, 2.049597641090739, 2.0484192035224518, 2.047957953495723, 2.0379233546999562, 8.043544751818581, 3.777077629586955, 10.704242227428246, 15.562056794641338, 3.1699195140464425, 25.355426647858124, 4.312762883842927, 4.763040945649756, 5.632617379968215, 5.25809240290824, 6.7116881121406555, 11.459935306933149, 22.966601509431385, 13.230173534514247, 11.59450540401688, 27.93189697572988, 9.28570393094161, 6.121863435104575, 7.5054057920632795, 7.6412366927915, 10.844562681988055, 8.657269251668172, 8.350381287456171, 7.997275106280223, 7.53640677600257, 8.253874764006706, 9.65447654159682, 10.088844691767422, 8.022081357717687, 8.16499172314186, 6.605653641710496, 6.558359844327075, 11.72635713593893, 5.748131902877349, 4.33057739352183, 4.237857997550877, 4.049083080525297, 3.983099525124395, 3.8960709897224395, 3.824575689532614, 3.515627948688121, 13.225847746698072, 3.1381330110005186, 3.0646702476239205, 3.026364951594084, 2.9861105536185284, 2.9246007708082136, 2.8248454596812875, 9.74712926500051, 2.7622990161898557, 2.5739730127720755, 2.49899282397412, 2.479218142078611, 2.4191307933711412, 2.248749905329602, 2.248399168153558, 2.247709872175638, 2.1090315404091444, 2.093834990402864, 2.0928625940654344, 3.86508283561206, 1.9630827192642377, 4.849217626863197, 3.504489420314286, 3.9465876352105274, 12.593934768981889, 5.222011150043068, 3.69565691946491, 5.573099812701572, 8.904449474826215, 6.435827829791153, 5.746672371573191, 17.76733359653757, 10.029939591845618, 8.815486638887146, 11.496915067733292, 11.62446348966767, 7.283115167312283, 6.312067069582687, 11.125108929659058, 9.069666919580042, 15.598263982996436, 7.125054859377304, 7.7018683757261055, 8.7293784138531, 5.557277294263046, 10.235302953365101, 7.018590388241317, 7.7749056454452266, 9.669312077100173, 7.784528434634128, 6.648861693440575, 6.017169800055445, 5.926984630069093, 5.864869301023215, 10.42650996765612, 12.782573428644126, 6.547247075200219, 4.171487751196938, 4.154932220549996, 4.753935076883497, 3.539319598680292, 3.52102921297814, 3.64406701187264, 3.312725900654426, 3.2253356362283165, 11.204572287184071, 3.084184045722301, 2.931937782656774, 2.90446218933769, 2.8259978633018097, 2.815562938328104, 2.6740393097097916, 2.440756862475643, 2.348713310691195, 2.3079433901424276, 2.235353297874308, 2.207226389833296, 4.660151048134588, 2.117777018468327, 2.117604227433949, 2.0464864139888927, 3.8323507053482455, 8.661702041351143, 1.8740755198865975, 33.95919972014479, 5.598584943100633, 5.508717400897442, 7.276287668848656, 3.1739768496024885, 6.371580930290461, 5.302278908942833, 6.712533191426619, 9.043391979733002, 5.666944714006528, 9.005653756423774, 9.64372458295499, 12.461210544947988, 6.4903811324107625, 9.521318422100032, 3.924308605691729, 10.953654572025957, 10.959346589793828, 6.286328314182474, 15.844282403977159, 8.961598491318016, 5.973276596564779, 7.606103258456181, 6.44125093675273, 10.849286141657336, 6.009650349428373, 6.964077253415569, 6.293132808581249, 10.337703684115937, 6.6390735400382725, 13.495311639214538, 6.001475387418343, 5.624767795528345, 5.483938754141452, 5.38328459205633, 5.188524869312657, 4.619265239553383, 4.229352208885635, 4.031128936554433, 4.023063392245068, 6.047966324902596, 3.3107280056941137, 3.29042065032368, 3.2528595046074216, 2.834091104426232, 2.824469586703682, 2.731774700929217, 2.6076080881545964, 2.8854113320740047, 2.4137946097821366, 2.4130625413465907, 2.1727125352064807, 2.085704299313508, 1.9690417159908804, 1.9678009387096667, 1.967065754391443, 1.9614950481443438, 1.990003242729906, 6.0020972520033755, 10.731788717695935, 4.92107193167484, 11.736958038218155, 6.769348266825064, 22.666208440921217, 5.2376078931975005, 9.692439148157098, 8.42644486443501, 11.580900202238603, 11.346139761635486, 4.416210709132761, 8.184900983247685, 4.520592450863929, 6.8518397847852865, 8.060746506016127, 8.693401884745523, 12.659192401476904, 7.473383597896664, 10.890446183792037, 7.040077801089717, 6.695136469109875, 6.897955504397558, 5.828242150141395, 5.190114625453826, 7.0591125251665865, 5.608343592780123, 3.9482382113647407, 3.8686270767813182, 7.048721001503164, 3.627050010817067, 3.443207510753374, 3.367931349538439, 6.423934681741526, 3.329682345087378, 3.0887831136734003, 2.938941273046822, 2.9434410988810416, 4.192653972140867, 2.8076317587296558, 5.929222775764582, 2.645489176618391, 2.576119097418394, 2.466561057854214, 2.401620823000533, 2.3817384842928293, 2.3032478616161893, 7.712191406929343, 2.182030712223161, 2.146987441750325, 2.144275941929166, 2.1454711223057212, 2.0741451170858363, 5.643534099131667, 1.8638948682125565, 4.649782425092126, 5.58663478572415, 11.571783275628045, 7.197743105162888, 5.048846607211363, 6.537237458198849, 4.232745768566707, 8.71204876376341, 8.080199647185616, 4.657221796658433, 5.221799470573021, 4.232471014457154, 11.243592400928188, 6.174686157798428, 7.184692608199251, 5.233528238649644, 13.84877078593342, 5.16067734812174, 8.058072567402865, 6.953117561543114, 10.341423382885486, 7.487045606436746, 6.347747629843392, 5.637669230233162, 4.908844842588327, 4.726672201030459, 7.3464285556094175, 5.264920887845915, 9.690299672751468, 6.129743029202949, 4.472315026022693, 3.8866780396627583, 3.820850090773167, 10.158732541699026, 3.262453768639866, 3.1572710202841527, 3.090042490073475, 3.0575436129568914, 3.064596663444931, 3.0959118315327068, 2.98130422704161, 2.5125482434504987, 2.4427136416409185, 2.409874099521869, 2.3078099872650886, 2.2411820867482715, 2.205174861545113, 6.369445831232543, 2.1320277861903953, 4.93560658000663, 2.0538045829194465, 2.0424526346955956, 1.9863378692663824, 1.9849327436559348, 1.9132114782632401, 12.078729897575787, 19.42996474433315, 6.848900325577274, 5.071797678568413, 2.955765007431709, 13.7107101937319, 10.390186048241121, 7.324209174427413, 6.254822556778131, 4.602263524599083, 9.903343479592678, 12.191611298908112, 19.477306837500148, 8.204666178381528, 9.295202858916205, 10.178466666699286, 5.500945009287701, 6.7945957543101985, 6.153918021806654, 4.0228776000047315, 10.448998554554628, 6.643191315164506, 6.881626103762658, 6.8488106933871125, 6.529116489426116, 5.930727600319776, 5.334760033944856, 5.879940797063831, 3.5623488250224313, 3.593520535211395, 3.5062189510690334, 3.383506836611118, 3.140389549628189, 3.059187535953048, 2.8630220589928386, 2.868556605791995, 2.7963372686921457, 5.537506174890297, 2.390906363858905, 2.226485333414488, 2.1947367449152844, 2.092203516673587, 2.088170367447404, 2.07360843921752, 2.0720524817251325, 2.024707040451356, 2.574018165858884, 1.9108532002393885, 2.237789520278128, 1.8264597169325785, 1.8077548055026131, 1.8065909081641274, 1.7777612753535526, 1.7713843348248106, 1.7713834778627258, 1.7713352023319355, 1.7708858685453512, 3.1925788262605876, 7.306091957199426, 3.356008351572624, 11.42136671653106, 3.0854711344089965, 2.632163614636274, 6.6353751560326115, 2.9371724173242906, 6.624361479315797, 4.513983510742879, 4.026983383981061, 3.740478392287056, 3.688717025389116, 3.95383481432701, 6.306060623380898, 2.860372617879952, 8.968571550680336, 4.497602966141545, 5.118587686627772, 5.546194627816395, 8.170166827535889, 6.880887366389173, 5.395333308443419, 5.171671346022915, 5.20671595352816, 4.60322011527324, 4.335706545999629, 4.662674430808501, 4.14991573769483, 4.844591770488961, 4.05642288814215, 4.066547323868731, 3.7986121292492845, 3.7235108286574765], \"Total\": [51.0, 56.0, 37.0, 43.0, 35.0, 48.0, 19.0, 15.0, 26.0, 13.0, 17.0, 38.0, 31.0, 35.0, 23.0, 110.0, 21.0, 19.0, 16.0, 32.0, 15.0, 45.0, 15.0, 12.0, 13.0, 11.0, 11.0, 28.0, 25.0, 16.0, 7.827949934484095, 6.572328284612035, 6.172914778943705, 6.213480766936751, 5.737825411229567, 5.382391802316187, 5.077705364805414, 4.89489064862342, 4.646266912674995, 4.5968072446041734, 4.4465076213933425, 4.642396865708104, 4.233188491209212, 8.124241730777536, 4.134292877403066, 3.6396293884136224, 3.5444226716626748, 10.983482245888295, 3.3208080156117092, 3.2732934070032442, 3.273111262413985, 3.275200041876776, 3.2634117189184817, 3.2203584434562664, 10.226790480766912, 6.007844955734847, 6.970291072678285, 3.1415779278464533, 6.8447884595854225, 2.8693056111982167, 11.997351153153755, 8.031512694952424, 8.290902887822414, 9.617212176458375, 7.944508766128768, 8.00357357943325, 14.017042050366468, 22.81435904515063, 43.931506284561884, 13.423315377035848, 9.63638570406026, 12.677228406002886, 110.58129262846164, 149.04442664085477, 59.19069939763692, 53.57277205895164, 21.983153092077135, 39.70531187025231, 22.243613955567728, 28.407769242006623, 45.765003109229994, 43.38307577207075, 38.06487423645, 32.46750937245925, 47.665209270452515, 20.3641083562164, 44.35731548844942, 137.7600493379127, 31.640123205199263, 32.08536336804063, 35.55936383615705, 24.375061025102372, 61.746500969309054, 40.563994136953916, 52.62400534674904, 48.498065178686126, 9.380392920599352, 11.808971023585869, 14.049128187410854, 6.112886125895766, 5.951056231892826, 5.833171451652233, 4.894842932261339, 4.717131107645087, 4.5131533027332855, 4.440749548750037, 4.310141352096254, 4.226330468562689, 4.025633787888582, 3.943149181288967, 3.9184055624464293, 3.9178358903982216, 4.519650348485443, 3.8821146077831497, 3.8820589385886506, 7.015828044391568, 6.858843571071944, 3.663563791552258, 3.983506722153628, 3.4214616754803893, 4.382736976863067, 3.2990104241862084, 3.2736631209421905, 3.258934281629216, 3.2228440149991697, 3.198396138290251, 6.025254190735266, 11.332649116037626, 26.114253982840488, 5.293422031182982, 11.649640175810037, 28.585224157384037, 19.004860456885794, 13.104909641476487, 24.9574619004458, 38.4428035790802, 56.59972344444861, 21.024566179724822, 12.712629003906887, 35.392846963353215, 29.865847196862347, 17.74400098043787, 37.77321274396264, 73.7395953472172, 21.900281450924883, 47.665209270452515, 20.639307505052077, 40.563994136953916, 27.546962044185314, 51.75286928837736, 137.7600493379127, 60.6429770273989, 110.58129262846164, 69.92024090857633, 46.749868605705615, 71.59671433139371, 149.04442664085477, 44.35731548844942, 39.70546008003075, 9.51851443370357, 7.360054524505559, 8.508362083323632, 6.12219902326316, 5.28527244222348, 15.718074897566211, 4.863721649036944, 8.57631133294836, 4.403263068641495, 10.109678927733544, 3.8785242360315215, 3.8018730871901334, 3.766605629237095, 4.3107211668400005, 3.70214922556716, 3.4246535726151817, 3.3991812585017147, 6.581690903209994, 3.236140251295946, 5.918948462509928, 3.135498565749553, 3.0482072963591045, 6.140207715182818, 2.938269064253005, 2.93545900165929, 2.9335460574802856, 5.3389894675568, 2.8572582218921765, 2.856583719965398, 2.8465723199344546, 21.168732542553823, 16.846336158055244, 19.20147243764371, 7.624097977566879, 11.067696404140948, 12.280044544785204, 4.455398662275016, 7.587788355165684, 27.223358150843307, 11.836446283122424, 41.13252511316085, 17.33104822287732, 73.7395953472172, 110.58129262846164, 8.471244757377363, 43.931506284561884, 21.646792059343724, 61.746500969309054, 69.92024090857633, 27.60162811670922, 20.072400992000016, 35.10167314417637, 137.7600493379127, 45.765003109229994, 59.19069939763692, 24.041096162311618, 26.832638115120265, 71.59671433139371, 149.04442664085477, 28.909636498798243, 11.89837999599278, 9.340144979820597, 7.851338375387458, 7.650291736671216, 5.902078944959124, 5.806080773679874, 5.431455883874743, 3.8667475679694054, 3.8478772327487967, 3.7252182983461624, 3.5590952597354932, 3.554844726292486, 3.554168822888877, 3.5151734277790156, 6.329746963227425, 3.397140488750187, 9.784815568529439, 6.418153804288531, 3.1460783569378896, 3.0613047645143903, 3.0353574518007225, 2.8783579140238946, 5.344415217574088, 2.8062695970619136, 2.806184214700527, 2.8055793711163237, 2.7350709640523934, 2.7340003650868674, 2.733585954313086, 2.7247173955804644, 10.767453800355584, 5.1509756639396915, 15.660079320294143, 23.45395314545435, 4.425594239909188, 48.0898059444645, 6.4955745487564664, 7.566225189771263, 9.337056192713638, 8.695571986030762, 12.058599983649394, 26.58285002623369, 71.59671433139371, 38.4428035790802, 33.950404678912165, 149.04442664085477, 24.81798313811221, 12.914481693461447, 19.606144772445855, 20.3641083562164, 48.498065178686126, 31.86837600316428, 34.02332933484227, 31.880449110132055, 27.546962044185314, 53.57277205895164, 110.58129262846164, 137.7600493379127, 60.6429770273989, 73.7395953472172, 24.06991612270659, 41.26516751402704, 12.643473474959787, 6.455942867052058, 5.01782410513643, 4.9336797116802575, 4.735150332663593, 4.686220101650505, 4.590904267959943, 4.510771919006338, 4.2178281628299885, 15.867598449002788, 3.825695623517192, 3.7508778265684968, 3.7127469304483376, 3.6735003021643515, 3.610933154398934, 3.516837512745003, 12.166003546053682, 3.452040774121809, 3.260675300308406, 3.1850153722867827, 3.175554012844901, 3.1079692988071717, 2.9348387634273405, 2.9344644137606175, 2.9338814653440903, 2.7950724780321266, 2.779894098563723, 2.7790240926537364, 5.155704728868212, 2.64939437999144, 6.557947284754983, 4.806885459810354, 5.479820930388819, 19.185017644597387, 7.614480301823276, 5.15544052393117, 8.733072753243526, 15.68045764258138, 11.368981462087882, 9.946379931252476, 52.62400534674904, 24.78260150615856, 20.619108697470992, 33.9285184114935, 43.38307577207075, 19.00987699068079, 15.550610926226184, 60.6429770273989, 39.70546008003075, 137.7600493379127, 24.375061025102372, 29.865847196862347, 41.26516751402704, 13.425767117361794, 110.58129262846164, 34.02332933484227, 61.746500969309054, 149.04442664085477, 71.59671433139371, 35.55936383615705, 23.259917623118834, 28.407769242006623, 33.950404678912165, 11.159126737939442, 13.735462083484277, 7.239413387472556, 4.861657065536601, 4.844563502706132, 5.650553743956239, 4.22749192026895, 4.212852753722282, 4.3982983008019945, 4.007149602611641, 3.9190895215872534, 13.670691534370428, 3.7726881397627303, 3.621372735412234, 3.592930105059871, 3.5143168746638223, 3.5053976766193946, 3.368186451442289, 3.129523683400019, 3.0431102112086874, 2.9960330452357464, 2.92414586730743, 2.8953105668468595, 6.1384327042857265, 2.8059440920206025, 2.805824651430363, 2.7349077386130407, 5.158160044401668, 11.797288943551047, 2.5622541795523284, 51.75286928837736, 7.992007576353206, 8.15793750779023, 11.227609222365535, 4.428370622160961, 10.082982505888921, 8.140534674900254, 10.779522082932457, 15.843543242785808, 9.355622253194582, 17.474646015391762, 23.716664858108874, 35.996008349338176, 13.425767117361794, 25.874958151300028, 5.932142764156714, 41.649159840780484, 46.263319747308515, 16.135904082719968, 149.04442664085477, 40.563994136953916, 17.0598306780726, 33.950404678912165, 22.05362532030819, 137.7600493379127, 19.82830783532546, 43.38307577207075, 27.546962044185314, 11.0719772376968, 7.343971516207219, 15.05595251046713, 6.716296654271674, 6.3236466003299725, 6.176440019250954, 6.080716527702572, 5.887375229000816, 5.312630411612794, 4.924141673137777, 4.719709654699989, 4.715661287490594, 7.140813454005039, 3.9992484504275505, 3.984552014621327, 3.9445288708089574, 3.5250158517547128, 3.517934336097656, 3.421757413010814, 3.297657783976592, 3.6645292323199254, 3.102419826939386, 3.1017670457628004, 2.861360016873794, 2.774174483849017, 2.6575194013166814, 2.6563915690729534, 2.6557124318350174, 2.651144026036578, 2.703228936347418, 8.171803682003075, 15.593240059818463, 7.010848564704781, 19.3685312917876, 10.76275520688227, 56.59972344444861, 8.868572233916009, 21.161179065395352, 18.777932304098712, 35.392846963353215, 41.26516751402704, 8.176160403686724, 28.836089271615315, 9.18082051083874, 22.328651792744207, 32.04525965057456, 44.35731548844942, 137.7600493379127, 37.02658443608727, 149.04442664085477, 61.746500969309054, 59.19069939763692, 69.92024090857633, 43.931506284561884, 17.16413441787993, 7.746213252485257, 6.298666083730036, 4.642113401096128, 4.555811168811109, 8.472001070096864, 4.362399969843862, 4.144848174645369, 4.055018931820333, 7.736820475102944, 4.016804706869705, 3.775953772324063, 3.6272720121502506, 3.641383311823158, 5.212501106428306, 3.4948639292467027, 7.4037037214902135, 3.340013060180889, 3.26332952926695, 3.1537561990081437, 3.090119988176558, 3.0706342087757474, 2.9903825052038293, 10.08512596607913, 2.86917150037367, 2.8341160095562703, 2.8314192846824144, 2.83409900533175, 2.7616751975973055, 7.59025897057207, 2.551274796297732, 6.456731111142825, 7.787182839306114, 19.794124699449895, 11.673542693801352, 7.756799591186212, 11.702548876776936, 6.803921457252114, 18.185757494296087, 19.275999095065924, 8.371525685677822, 10.190129747025106, 7.335069781936762, 45.765003109229994, 15.957192942643328, 23.259917623118834, 12.744587912831506, 149.04442664085477, 13.323347852559293, 53.57277205895164, 35.55936383615705, 137.7600493379127, 69.92024090857633, 39.70531187025231, 60.6429770273989, 37.77321274396264, 110.58129262846164, 8.042058714877285, 5.9536544324398735, 11.038161180590285, 6.989934264918585, 5.160619673898419, 4.57495192896816, 4.509899751071515, 12.044004297684724, 3.9505395174886777, 3.8454925696791467, 3.777944516082674, 3.7454667975080445, 3.758700241586325, 3.8058196515867806, 3.669932482441549, 3.2020170198717146, 3.1314612480887782, 3.103498911344011, 2.995681555652985, 2.929010372346789, 2.8969417697434308, 8.380787730542139, 2.821946244610442, 6.556010102921194, 2.7419883432114043, 2.7322440924785285, 2.6741685204601127, 2.67289207433509, 2.601698891618091, 16.437049446098985, 37.02658443608727, 11.64976056528903, 8.279242230608594, 4.359378597245131, 31.80137808985666, 25.041546830304075, 17.97462697292799, 14.305249460918253, 9.0217336801921, 32.04525965057456, 46.749868605705615, 137.7600493379127, 28.909636498798243, 38.06487423645, 46.263319747308515, 14.846808935896819, 28.407769242006623, 24.382855455292045, 7.722056028805239, 149.04442664085477, 41.649159840780484, 60.6429770273989, 61.746500969309054, 59.19069939763692, 48.498065178686126, 73.7395953472172, 6.56808795678122, 4.249231520056206, 4.296383185820806, 4.202093584909261, 4.074194693082231, 3.827057984361181, 3.747213404014394, 3.550279873548439, 3.5582975175622957, 3.4878551914537264, 6.9523859484372466, 3.0802523533076966, 2.913187043221007, 2.8814624747994264, 2.779053473579256, 2.7749586190919597, 2.7603223845537386, 2.7589487636454475, 2.711342377164315, 3.474324544210617, 2.5976382601956267, 3.0581254203159656, 2.5132264938989097, 2.4946540543860474, 2.4935509731251297, 2.464498972094327, 2.458025217193268, 2.458024682415947, 2.457975976227579, 2.4575704541907077, 4.442688540401841, 10.329670278620885, 4.751456724895089, 17.121227085146913, 4.393770709571693, 3.795917188112715, 10.936028385894858, 4.330901749409236, 12.428546554069115, 7.711109005774674, 6.854186444672163, 6.389532188916646, 6.627689164015193, 7.547307301101545, 15.881778616491905, 4.588428243075729, 35.996008349338176, 10.89341768036849, 15.120982741773307, 22.59253618752082, 69.92024090857633, 47.665209270452515, 26.832638115120265, 31.86837600316428, 44.35731548844942, 32.08536336804063, 24.81798313811221, 38.06487423645, 23.259917623118834, 73.7395953472172, 37.77321274396264, 71.59671433139371, 19.304731379975614, 16.413190355321177], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.7117, -5.9038, -5.974, -5.9677, -6.056, -6.1301, -6.1959, -6.2404, -6.2996, -6.3163, -6.3509, -6.3138, -6.4093, -5.7798, -6.4609, -6.5922, -6.6293, -5.5099, -6.7074, -6.7245, -6.7246, -6.7242, -6.7304, -6.7452, -5.5902, -6.1248, -5.9778, -6.7772, -6.0167, -6.8941, -5.4769, -5.8805, -5.854, -5.7286, -5.9044, -5.8978, -5.4412, -5.0831, -4.5419, -5.5288, -5.8132, -5.6134, -4.1232, -4.0124, -4.6381, -4.7422, -5.3103, -4.9446, -5.304, -5.2256, -4.9824, -5.0512, -5.1317, -5.2828, -5.1257, -5.506, -5.1981, -4.79, -5.3786, -5.3776, -5.3915, -5.547, -5.3535, -5.4555, -5.4639, -5.527, -5.4626, -5.2594, -5.0869, -5.9335, -5.9645, -5.9865, -6.1878, -6.2313, -6.2833, -6.302, -6.3376, -6.361, -6.4187, -6.4437, -6.4513, -6.4515, -6.309, -6.4626, -6.4626, -5.875, -5.9038, -6.5334, -6.4618, -6.6181, -6.3747, -6.6647, -6.6746, -6.6794, -6.6946, -6.7031, -6.088, -5.5044, -4.7226, -6.2439, -5.5557, -4.7971, -5.1765, -5.5074, -5.0872, -4.823, -4.5518, -5.2728, -5.6408, -4.994, -5.1365, -5.4735, -5.0004, -4.6084, -5.359, -4.8964, -5.4332, -5.0842, -5.2926, -4.9783, -4.5298, -5.0295, -4.8734, -5.0559, -5.2755, -5.1673, -5.3047, -5.4233, -5.4418, -5.4213, -5.6875, -5.556, -5.8881, -6.0542, -4.9713, -6.1482, -5.5937, -6.2698, -5.4503, -6.4168, -6.4411, -6.4523, -6.323, -6.4758, -6.5717, -6.5799, -5.9278, -6.6409, -6.0385, -6.6827, -6.7173, -6.0236, -6.7649, -6.7663, -6.7671, -6.1731, -6.8014, -6.8018, -6.81, -4.8231, -5.0821, -4.9825, -5.8778, -5.5704, -5.503, -6.4129, -5.9999, -5.038, -5.6851, -4.8262, -5.4475, -4.4878, -4.282, -5.9588, -4.999, -5.4484, -4.9339, -4.9135, -5.3733, -5.5631, -5.3591, -4.8546, -5.2687, -5.26, -5.5729, -5.5621, -5.3426, -5.2433, -5.6205, -5.0377, -5.3053, -5.4858, -5.5125, -5.802, -5.8204, -5.8964, -6.2961, -6.3022, -6.3426, -6.3979, -6.3993, -6.4, -6.4174, -5.8315, -6.4564, -5.4041, -5.84, -6.553, -6.5887, -6.5992, -6.6682, -6.0549, -6.7016, -6.7017, -6.702, -6.7357, -6.7363, -6.7365, -6.7414, -5.3685, -6.1244, -5.0827, -4.7085, -6.2997, -4.2204, -5.9918, -5.8925, -5.7248, -5.7936, -5.5495, -5.0145, -4.3193, -4.8709, -5.0028, -4.1236, -5.2249, -5.6415, -5.4377, -5.4198, -5.0697, -5.295, -5.3311, -5.3743, -5.4336, -5.3427, -5.1859, -5.1419, -5.3712, -5.3535, -5.5654, -5.5726, -4.9218, -5.6347, -5.9179, -5.9396, -5.9851, -6.0016, -6.0237, -6.0422, -6.1264, -4.8014, -6.24, -6.2637, -6.2763, -6.2896, -6.3105, -6.3452, -5.1066, -6.3676, -6.4382, -6.4677, -6.4757, -6.5002, -6.5732, -6.5734, -6.5737, -6.6374, -6.6446, -6.6451, -6.0316, -6.7091, -5.8048, -6.1296, -6.0108, -4.8504, -5.7307, -6.0765, -5.6657, -5.1971, -5.5217, -5.635, -4.5063, -5.078, -5.2071, -4.9415, -4.9305, -5.3981, -5.5412, -4.9744, -5.1787, -4.6365, -5.42, -5.3422, -5.2169, -5.6685, -5.0578, -5.4351, -5.3327, -5.1147, -5.3315, -5.4892, -5.589, -5.6041, -5.6146, -4.9143, -4.7106, -5.3796, -5.8304, -5.8344, -5.6997, -5.9947, -5.9999, -5.9656, -6.0609, -6.0876, -4.8423, -6.1324, -6.183, -6.1924, -6.2198, -6.2235, -6.2751, -6.3663, -6.4048, -6.4223, -6.4543, -6.4669, -5.7196, -6.5083, -6.5084, -6.5425, -5.9152, -5.0997, -6.6305, -3.7335, -5.5361, -5.5523, -5.274, -6.1037, -5.4068, -5.5905, -5.3547, -5.0566, -5.524, -5.0608, -4.9923, -4.736, -5.3883, -5.0051, -5.8915, -4.865, -4.8645, -5.4203, -4.4958, -5.0657, -5.4714, -5.2297, -5.3959, -4.8746, -5.4653, -5.3179, -5.4192, -4.9146, -5.3574, -4.6481, -5.4584, -5.5232, -5.5486, -5.5671, -5.604, -5.7202, -5.8084, -5.8564, -5.8584, -5.4507, -6.0533, -6.0594, -6.0709, -6.2087, -6.2121, -6.2455, -6.292, -6.1908, -6.3692, -6.3695, -6.4744, -6.5153, -6.5729, -6.5735, -6.5739, -6.5767, -6.5623, -5.4583, -4.8772, -5.6569, -4.7877, -5.338, -4.1295, -5.5946, -4.9791, -5.119, -4.8011, -4.8215, -5.7651, -5.1481, -5.7418, -5.3259, -5.1634, -5.0879, -4.712, -5.2391, -4.8625, -5.2988, -5.349, -5.3192, -5.4877, -5.6037, -5.2815, -5.5116, -5.8626, -5.8829, -5.283, -5.9474, -5.9994, -6.0215, -5.3758, -6.033, -6.1081, -6.1578, -6.1563, -5.8025, -6.2035, -5.4559, -6.263, -6.2896, -6.333, -6.3597, -6.368, -6.4015, -5.193, -6.4556, -6.4718, -6.473, -6.4725, -6.5063, -5.5053, -6.6132, -5.699, -5.5155, -4.7873, -5.2621, -5.6167, -5.3583, -5.793, -5.0711, -5.1464, -5.6974, -5.583, -5.7931, -4.816, -5.4154, -5.2639, -5.5808, -4.6076, -5.5948, -5.1492, -5.2966, -4.8997, -5.2227, -5.3877, -5.5064, -5.6448, -5.6826, -5.2107, -5.5439, -4.9338, -5.3918, -5.707, -5.8474, -5.8644, -4.8866, -6.0224, -6.0552, -6.0767, -6.0873, -6.085, -6.0748, -6.1126, -6.2836, -6.3118, -6.3253, -6.3686, -6.3979, -6.4141, -5.3534, -6.4478, -5.6084, -6.4852, -6.4908, -6.5186, -6.5193, -6.5561, -4.7135, -4.2381, -5.2808, -5.5812, -6.1212, -4.5867, -4.8641, -5.2137, -5.3716, -5.6784, -4.912, -4.7042, -4.2357, -5.1002, -4.9754, -4.8846, -5.5, -5.2888, -5.3878, -5.8129, -4.8584, -5.3113, -5.2761, -5.2808, -5.3287, -5.4248, -5.5307, -5.3407, -5.8418, -5.8331, -5.8577, -5.8933, -5.9679, -5.9941, -6.0603, -6.0584, -6.0839, -5.4007, -6.2406, -6.3118, -6.3262, -6.374, -6.3759, -6.3829, -6.3837, -6.4068, -6.1668, -6.4647, -6.3067, -6.5098, -6.5201, -6.5208, -6.5369, -6.5405, -6.5405, -6.5405, -6.5407, -5.9514, -5.1235, -5.9015, -4.6767, -5.9855, -6.1444, -5.2198, -6.0348, -5.2215, -5.605, -5.7192, -5.793, -5.8069, -5.7375, -5.2707, -6.0613, -4.9185, -5.6087, -5.4793, -5.3991, -5.0117, -5.1835, -5.4267, -5.469, -5.4623, -5.5855, -5.6453, -5.5726, -5.6891, -5.5344, -5.7119, -5.7094, -5.7776, -5.7976], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.9288, 1.9115, 1.904, 1.9037, 1.8951, 1.885, 1.8774, 1.8695, 1.8625, 1.8565, 1.8551, 1.8492, 1.8459, 1.8235, 1.818, 1.8141, 1.8035, 1.7919, 1.7906, 1.7879, 1.7879, 1.7876, 1.785, 1.7835, 1.783, 1.7803, 1.7787, 1.7762, 1.758, 1.75, 1.7366, 1.7343, 1.729, 1.7061, 1.7213, 1.7205, 1.6168, 1.4877, 1.3737, 1.5724, 1.6194, 1.545, 0.8693, 0.6815, 0.9793, 0.975, 1.2976, 1.0721, 1.2921, 1.1259, 0.8922, 0.8769, 0.9272, 0.9352, 0.7083, 1.1784, 0.7078, -0.0174, 0.8651, 0.8522, 0.7355, 0.9577, 0.2217, 0.5398, 0.2711, 0.2897, 1.9969, 1.9699, 1.9688, 1.9543, 1.9501, 1.9482, 1.9222, 1.9157, 1.9079, 1.9054, 1.8997, 1.8958, 1.8868, 1.8825, 1.8812, 1.8811, 1.8808, 1.8792, 1.8792, 1.875, 1.8688, 1.8664, 1.8542, 1.85, 1.8458, 1.8399, 1.8376, 1.8374, 1.8333, 1.8324, 1.8142, 1.7661, 1.7131, 1.7878, 1.6872, 1.5482, 1.577, 1.6178, 1.3938, 1.226, 1.1104, 1.3797, 1.5148, 1.1376, 1.1649, 1.3486, 1.0662, 0.7893, 1.2526, 0.9376, 1.2378, 0.9112, 1.0897, 0.7734, 0.2429, 0.5637, 0.119, 0.3949, 0.5779, 0.2598, -0.6107, 0.4826, 0.5749, 2.0236, 2.0147, 2.0011, 1.9982, 1.9791, 1.972, 1.9682, 1.9554, 1.946, 1.9344, 1.926, 1.9216, 1.9197, 1.914, 1.9134, 1.8955, 1.8947, 1.8861, 1.8829, 1.8816, 1.8727, 1.8663, 1.8598, 1.8554, 1.855, 1.8549, 1.85, 1.8469, 1.8468, 1.8421, 1.8226, 1.792, 1.7607, 1.7891, 1.7238, 1.6872, 1.7912, 1.6717, 1.3561, 1.5419, 1.1552, 1.3981, 0.9098, 0.7105, 1.6027, 0.9166, 1.175, 0.6413, 0.5374, 1.007, 1.1358, 0.7808, -0.0819, 0.6059, 0.3574, 0.9455, 0.8464, 0.0845, -0.5493, 0.7135, 2.184, 2.1585, 2.1517, 2.1509, 2.1209, 2.1189, 2.1096, 2.0496, 2.0484, 2.0404, 2.0307, 2.0306, 2.0301, 2.0237, 2.0214, 2.0188, 2.0133, 1.9991, 1.999, 1.9906, 1.9887, 1.9728, 1.9672, 1.9647, 1.9647, 1.9646, 1.9563, 1.9561, 1.956, 1.9544, 1.9532, 1.9346, 1.8643, 1.8346, 1.9111, 1.6047, 1.8353, 1.782, 1.7394, 1.7418, 1.6589, 1.4034, 1.1078, 1.1781, 1.1705, 0.5703, 1.2617, 1.4983, 1.2846, 1.2646, 0.747, 0.9416, 0.8401, 0.8619, 0.9487, 0.3745, -0.1935, -0.3693, 0.222, 0.0441, 0.9518, 0.4055, 2.2393, 2.1984, 2.1673, 2.1625, 2.158, 2.152, 2.1505, 2.1495, 2.1325, 2.1325, 2.1165, 2.1125, 2.1102, 2.1074, 2.1038, 2.0955, 2.0929, 2.0917, 2.0781, 2.072, 2.067, 2.064, 2.0483, 2.0483, 2.0481, 2.0329, 2.0311, 2.031, 2.0264, 2.0147, 2.0127, 1.9986, 1.9863, 1.8936, 1.9374, 1.9817, 1.8654, 1.7487, 1.7456, 1.766, 1.2288, 1.41, 1.4649, 1.2324, 0.9976, 1.3552, 1.4129, 0.6188, 0.838, 0.1362, 1.0846, 0.9593, 0.7612, 1.4325, -0.0653, 0.7361, 0.2424, -0.4207, 0.0957, 0.6378, 0.9624, 0.7474, 0.5586, 2.3716, 2.3676, 2.339, 2.2864, 2.286, 2.2667, 2.2619, 2.2601, 2.2514, 2.2492, 2.2447, 2.2406, 2.238, 2.2283, 2.2268, 2.2215, 2.2204, 2.2087, 2.191, 2.1805, 2.1786, 2.1709, 2.1682, 2.164, 2.1582, 2.1581, 2.1496, 2.1424, 2.1306, 2.1268, 2.0182, 2.0836, 2.0469, 2.0058, 2.1065, 1.9805, 2.0108, 1.9659, 1.8788, 1.9382, 1.7766, 1.5397, 1.3787, 1.7127, 1.4398, 2.0263, 1.1039, 0.9994, 1.4969, 0.1981, 0.9296, 1.3901, 0.9436, 1.2088, -0.1019, 1.2458, 0.6102, 0.9631, 2.3791, 2.3469, 2.3383, 2.3352, 2.3306, 2.3288, 2.3259, 2.3214, 2.3079, 2.2957, 2.2901, 2.2889, 2.2817, 2.2588, 2.2564, 2.255, 2.2296, 2.2282, 2.2226, 2.213, 2.2087, 2.1968, 2.1967, 2.1724, 2.1625, 2.1479, 2.1477, 2.1476, 2.1465, 2.1415, 2.1392, 2.0741, 2.0938, 1.9469, 1.9841, 1.5326, 1.9211, 1.6669, 1.6465, 1.3306, 1.1566, 1.8318, 1.1884, 1.7393, 1.2664, 1.0676, 0.8181, 0.0606, 0.8475, -0.1686, 0.2763, 0.2684, 0.1316, 0.4278, 1.2517, 2.3695, 2.3463, 2.3004, 2.2988, 2.2784, 2.2777, 2.2769, 2.2767, 2.2764, 2.2747, 2.2615, 2.2519, 2.2496, 2.2446, 2.2434, 2.2403, 2.2292, 2.2259, 2.2166, 2.2103, 2.2083, 2.2013, 2.1941, 2.1886, 2.1847, 2.1844, 2.184, 2.1761, 2.166, 2.1484, 2.134, 2.1302, 1.9255, 1.9788, 2.0329, 1.8801, 1.9877, 1.7264, 1.5929, 1.8759, 1.7938, 1.9125, 1.0586, 1.5129, 1.2876, 1.5723, 0.0863, 1.5139, 0.568, 0.8303, -0.127, 0.2282, 0.629, 0.0868, 0.4218, -0.6902, 2.4028, 2.3703, 2.363, 2.3619, 2.3501, 2.3302, 2.3275, 2.323, 2.3019, 2.2961, 2.2923, 2.2903, 2.2891, 2.2868, 2.2854, 2.2508, 2.2449, 2.2403, 2.2324, 2.2256, 2.2204, 2.2188, 2.2129, 2.2094, 2.2043, 2.2023, 2.1959, 2.1957, 2.1859, 2.1852, 1.8484, 1.9621, 2.0032, 2.1047, 1.6519, 1.6136, 1.5955, 1.666, 1.8202, 1.319, 1.1492, 0.537, 1.2338, 1.0835, 0.9792, 1.5004, 1.0627, 1.1165, 1.8412, -0.1645, 0.6576, 0.3171, 0.2943, 0.2888, 0.3919, -0.133, 2.4753, 2.4096, 2.4073, 2.4049, 2.4002, 2.3882, 2.3831, 2.3708, 2.3705, 2.365, 2.3584, 2.3326, 2.3171, 2.3137, 2.3021, 2.3016, 2.2999, 2.2996, 2.2939, 2.286, 2.2789, 2.2736, 2.2668, 2.2639, 2.2637, 2.2593, 2.2584, 2.2584, 2.2584, 2.2583, 2.2555, 2.2396, 2.2383, 2.1811, 2.2325, 2.2198, 2.0863, 2.1976, 1.9567, 2.0505, 2.0541, 2.0505, 2.0, 1.9395, 1.6623, 2.1134, 1.1963, 1.7013, 1.5028, 1.1815, 0.4391, 0.6505, 0.9819, 0.7675, 0.4436, 0.6443, 0.8413, 0.4863, 0.8623, -0.1367, 0.3547, -0.2823, 0.9602, 1.1025]}, \"token.table\": {\"Topic\": [1, 7, 1, 2, 3, 3, 8, 10, 8, 6, 7, 1, 2, 3, 6, 10, 6, 10, 8, 4, 8, 1, 9, 9, 5, 1, 3, 2, 5, 5, 4, 6, 6, 2, 9, 3, 6, 7, 2, 2, 4, 6, 3, 4, 6, 1, 2, 6, 2, 7, 2, 3, 4, 5, 9, 10, 2, 6, 7, 8, 1, 2, 7, 9, 10, 3, 5, 9, 4, 1, 10, 5, 1, 2, 9, 6, 10, 3, 9, 10, 4, 5, 6, 7, 9, 2, 4, 8, 2, 3, 4, 5, 8, 9, 10, 8, 1, 7, 9, 10, 10, 9, 3, 7, 1, 3, 4, 5, 8, 9, 2, 9, 6, 3, 4, 6, 10, 2, 3, 4, 6, 7, 9, 10, 6, 7, 8, 10, 1, 5, 2, 2, 3, 5, 6, 7, 8, 1, 3, 5, 7, 8, 9, 5, 1, 5, 6, 9, 10, 1, 2, 3, 4, 7, 8, 9, 10, 1, 8, 8, 7, 5, 6, 2, 3, 4, 5, 7, 9, 4, 7, 8, 6, 8, 10, 1, 2, 5, 8, 10, 2, 4, 3, 10, 4, 4, 6, 4, 8, 9, 7, 4, 10, 9, 3, 4, 7, 9, 10, 5, 7, 10, 5, 9, 5, 4, 1, 4, 7, 2, 6, 8, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 8, 3, 10, 2, 3, 9, 4, 8, 1, 5, 8, 1, 3, 4, 6, 7, 8, 10, 3, 10, 1, 5, 7, 8, 9, 3, 3, 8, 4, 1, 2, 4, 5, 9, 10, 3, 9, 3, 4, 5, 8, 10, 9, 4, 9, 10, 3, 6, 8, 10, 7, 7, 2, 3, 4, 9, 10, 10, 1, 5, 6, 10, 3, 3, 10, 9, 5, 1, 2, 6, 10, 1, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 5, 6, 7, 10, 1, 10, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 7, 9, 1, 3, 1, 2, 10, 6, 1, 5, 8, 10, 3, 9, 2, 3, 4, 5, 6, 8, 6, 7, 9, 10, 2, 10, 2, 5, 6, 7, 8, 10, 1, 3, 5, 6, 10, 2, 1, 7, 2, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 3, 5, 9, 10, 1, 2, 3, 4, 5, 6, 4, 6, 1, 2, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 5, 6, 5, 7, 8, 9, 1, 2, 6, 8, 10, 8, 1, 2, 3, 5, 9, 6, 7, 10, 4, 5, 9, 7, 1, 4, 6, 1, 3, 8, 1, 2, 5, 6, 7, 8, 9, 10, 4, 5, 6, 4, 9, 1, 2, 4, 5, 6, 7, 8, 9, 10, 6, 7, 3, 2, 8, 1, 5, 8, 1, 3, 5, 6, 8, 9, 10, 2, 1, 1, 5, 1, 4, 1, 2, 4, 6, 7, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 5, 7, 1, 8, 10, 2, 1, 8, 1, 5, 8, 8, 1, 6, 7, 4, 5, 5, 8, 7, 2, 3, 8, 10, 1, 2, 3, 4, 7, 9, 4, 5, 6, 3, 7, 9, 9, 10, 10, 7, 5, 10, 1, 2, 3, 4, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 3, 10, 1, 9, 1, 2, 3, 4, 6, 7, 8, 5, 6, 8, 7, 1, 3, 6, 8, 9, 10, 2, 1, 2, 3, 4, 7, 9, 4, 2, 3, 5, 7, 1, 7, 1, 2, 3, 4, 5, 6, 8, 10, 1, 3, 4, 7, 10, 1, 2, 3, 4, 5, 1, 3, 4, 3, 5, 1, 5, 1, 3, 6, 9, 4, 6, 8, 9, 2, 9, 1, 3, 4, 6, 7, 8, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 6, 1, 8, 3, 9, 1, 3, 7, 9, 1, 3, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 4, 1, 3, 5, 8, 9, 10, 3, 1, 7, 9, 2, 3, 5, 6, 9, 10, 7, 3, 5, 2, 3, 5, 8, 9, 6, 1, 7, 8, 9, 5, 7, 4, 6, 7, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 1, 4, 5, 6, 7, 8, 9, 10, 8, 6, 4, 9, 1, 2, 3, 4, 7, 8, 1, 3, 8, 2, 4, 8, 1, 4, 8, 9, 2, 8, 2, 3, 4, 2, 3, 4, 6, 10, 6, 1, 3, 9, 4, 5, 2, 7, 9, 2, 8, 3, 8, 1, 2, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 9, 10, 9, 10, 1, 6, 1, 2, 3, 4, 7, 9, 10, 9, 6, 3, 9, 2, 8, 6, 8, 1, 2, 4, 8, 10, 3, 3, 3, 6, 4, 5, 7, 9, 2, 2, 9, 10, 6, 1, 3, 4, 5, 6, 7, 4, 6, 3, 1, 8, 10, 8, 1, 2, 3, 6, 7, 9, 2, 4, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 2, 3, 4, 6, 8, 2, 3, 5, 6, 8, 9, 6, 9, 9, 3, 6, 8, 9, 10, 2, 9, 10, 5, 4, 6, 7, 4, 5, 6, 7, 9, 1, 3, 4, 6, 7, 1, 3, 7, 2, 3, 1, 2, 6, 7, 9, 2, 8, 9, 6, 1, 3, 6, 4, 7, 1, 5, 6, 8, 5, 6, 8, 9, 4, 8, 5, 1, 2, 6, 3, 7, 9, 2, 10, 2, 8, 5, 1, 2, 3, 5, 1, 3, 4, 5, 6, 7, 9, 10, 10, 9, 5, 10, 8, 2, 3, 9, 10, 10, 3, 5, 6, 8, 9, 10, 2, 3, 4, 5, 7, 9, 10, 10, 6, 8, 1, 2, 3, 4, 6, 7, 8, 9, 6, 2, 5, 10, 4, 1, 3, 4, 5, 1, 2, 4, 5, 7, 9, 9, 9, 4, 3, 8, 7, 6, 7, 8, 9, 2, 9, 1, 2, 3, 4, 7, 8, 1, 3, 2, 6, 7, 10, 6, 3, 6, 8, 10, 1, 2, 5, 7, 3, 5, 7, 1, 2, 3, 6, 7, 10, 2, 3, 4, 5, 6, 8, 9, 3, 5, 4, 8, 10, 2, 3, 4, 6, 8, 10, 2, 9, 1, 6, 9, 5, 1, 2, 3, 5, 8, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 6, 10, 6, 8, 1, 2, 3, 5, 6, 7, 9, 3, 4, 8, 10, 1, 1, 2, 3, 4, 6, 7, 8, 9, 10, 2, 4, 6, 7, 10, 4, 1, 3, 4, 8, 10, 7, 2, 10, 2, 6, 1, 2, 3, 5, 8, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 3, 8, 1, 4, 7, 8, 9, 4, 5, 10, 9, 1, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 4, 8, 7, 2, 3, 5, 7, 5, 6, 7, 9, 5, 7, 10, 2, 4, 6, 8, 9, 10, 2, 1, 3, 10, 7, 1, 2, 8, 10, 5, 3, 10, 4, 1, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 3, 4, 1, 2, 5, 10, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 6, 7, 8, 8, 1, 2, 3, 4, 5, 9, 10, 9, 2, 10, 3, 7, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 4, 9, 6, 4, 4, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 3, 6, 10, 1, 3, 5, 7, 9], \"Freq\": [0.8171786230045632, 0.8482373427902774, 0.7822591080794369, 0.09778238850992961, 0.09778238850992961, 0.1350675334423999, 0.8104052006543994, 0.9310156536318853, 0.6472240585001282, 0.1426360861699926, 0.713180430849963, 0.11112343238673418, 0.16668514858010128, 0.08334257429005064, 0.33337029716020256, 0.2500277228701519, 0.38904909757511774, 0.6484151626251962, 0.6688107613389334, 0.8429108470176085, 0.7237900819507529, 0.7173301585063915, 0.1434660317012783, 0.7751007151779377, 0.6435070001391563, 0.15193663979453734, 0.7596831989726868, 0.9308548555368954, 0.9491062739812618, 0.7548895004474425, 0.8440792065587811, 0.7486618413359871, 0.8848690281634675, 0.7657288574420278, 0.768728466788917, 0.7890847303946231, 0.9461874973247786, 0.9097366059562276, 0.8768182386782941, 0.772785794202978, 0.7758458361364997, 0.7127725765055318, 0.08476523757152249, 0.16953047514304498, 0.7628871381437023, 0.7877574046979559, 0.9164046174478003, 0.7654839174954552, 0.3614109864369557, 0.6195616910347811, 0.2421337950856278, 0.2421337950856278, 0.13836216862035874, 0.034590542155089686, 0.2767243372407175, 0.06918108431017937, 0.27051640213394146, 0.6569684051824293, 0.03864520030484878, 0.01932260015242439, 0.24040906178415625, 0.5208863005323385, 0.040068176964026044, 0.08013635392805209, 0.08013635392805209, 0.6561233556487043, 0.8712880440387569, 0.8704238862432645, 0.9244956039145377, 0.30176430283709293, 0.6035286056741859, 0.8308101733606785, 0.15261455856743064, 0.6104582342697226, 0.15261455856743064, 0.6839604078443635, 0.84309982096585, 0.8906943179261578, 0.06362102270901128, 0.06362102270901128, 0.06990440835946178, 0.2796176334378471, 0.06990440835946178, 0.06990440835946178, 0.41942645015677066, 0.25783829741747255, 0.12891914870873628, 0.6445957435436813, 0.051800772583516945, 0.20720309033406778, 0.31080463550110166, 0.10360154516703389, 0.051800772583516945, 0.051800772583516945, 0.20720309033406778, 0.697065337411698, 0.15650598047453476, 0.15650598047453476, 0.15650598047453476, 0.626023921898139, 0.7376419949190336, 0.9369094484451528, 0.9279189827376897, 0.8510600026115157, 0.0798672707222581, 0.0798672707222581, 0.19966817680564528, 0.0798672707222581, 0.11980090608338717, 0.39933635361129055, 0.8298405082541167, 0.16596810165082335, 0.9464552354326246, 0.09144087457653151, 0.18288174915306302, 0.09144087457653151, 0.6400861220357206, 0.4566151363126744, 0.04566151363126744, 0.27396908178760465, 0.04566151363126744, 0.04566151363126744, 0.04566151363126744, 0.04566151363126744, 0.9669291730339852, 0.2649951724780169, 0.13249758623900845, 0.5299903449560338, 0.9656423227263006, 0.6815553770634863, 0.8850242146143937, 0.35828405916561584, 0.08957101479140396, 0.17914202958280792, 0.04478550739570198, 0.31349855176991387, 0.04478550739570198, 0.5698165779837348, 0.13149613338086188, 0.043832044460287295, 0.043832044460287295, 0.043832044460287295, 0.13149613338086188, 0.8447461472147053, 0.9129184879653619, 0.09917700436512257, 0.5950620261907353, 0.09917700436512257, 0.09917700436512257, 0.3116685912293807, 0.12466743649175227, 0.0935005773688142, 0.15583429561469034, 0.0935005773688142, 0.031166859122938068, 0.062333718245876135, 0.15583429561469034, 0.3925367094729834, 0.4906708868412293, 0.7398239195527686, 0.644793748367441, 0.24516000497554036, 0.7354800149266211, 0.2050621187156651, 0.12303727122939906, 0.04101242374313302, 0.2050621187156651, 0.12303727122939906, 0.24607454245879812, 0.6426008236602692, 0.3213004118301346, 0.10710013727671153, 0.9094426358645643, 0.1936170222334551, 0.6776595778170927, 0.2812198791315811, 0.25309789121842297, 0.19685391539210675, 0.19685391539210675, 0.08436596373947432, 0.30790193923382597, 0.6158038784676519, 0.8143046997639118, 0.16286093995278236, 0.8534429556994813, 0.6778750688317868, 0.22595835627726227, 0.6589009801180096, 0.2415680015504249, 0.6039200038760623, 0.7543913044173504, 0.7765519119033509, 0.19413797797583773, 0.6386794667252096, 0.18723518128499303, 0.1560293177374942, 0.24964690837999073, 0.3120586354749884, 0.062411727094997684, 0.8530391265243314, 0.22508892777560094, 0.6752667833268029, 0.719451867261178, 0.7482531820883643, 0.8080270635730432, 0.731242452677249, 0.18711121035500594, 0.7484448414200238, 0.8933494615941215, 0.11945253918420397, 0.23890507836840794, 0.5972626959210198, 0.11945253918420397, 0.798148244653284, 0.16635542279456547, 0.04158885569864137, 0.08317771139728274, 0.5198606962330171, 0.020794427849320684, 0.04158885569864137, 0.04158885569864137, 0.020794427849320684, 0.020794427849320684, 0.6513312443025879, 0.2275949443200418, 0.6827848329601254, 0.7556548441511802, 0.18891371103779506, 0.6676277043619374, 0.8611661109962508, 0.9193064853226446, 0.919283332412075, 0.6870434003623246, 0.3435217001811623, 0.32776136744050094, 0.2185075782936673, 0.08740303131746692, 0.0655522734881002, 0.04370151565873346, 0.24035833612303403, 0.02185075782936673, 0.7734900744283033, 0.8450038044469873, 0.3307944221051002, 0.09451269203002863, 0.47256346015014317, 0.09451269203002863, 0.047256346015014314, 0.9455256975954576, 0.8447446420034805, 0.16894892840069609, 0.712711585192007, 0.12593047972115456, 0.06296523986057728, 0.06296523986057728, 0.18889571958173185, 0.12593047972115456, 0.3777914391634637, 0.8162017128631388, 0.11660024469473412, 0.15011242085192714, 0.2251686312778907, 0.15011242085192714, 0.3752810521298178, 0.07505621042596357, 0.6903832244364135, 0.7024230066156132, 0.25542654786022295, 0.802069025881364, 0.7558317423037242, 0.09447896778796552, 0.14171845168194827, 0.719669491434477, 0.7501409420260642, 0.8186590445345614, 0.08292836658948212, 0.08292836658948212, 0.5804985661263748, 0.24878509976844634, 0.9413466837756796, 0.801714368564909, 0.7256379963783393, 0.8107538862991421, 0.7128029183792484, 0.8136776027687359, 0.9084172209665613, 0.6817687402248808, 0.7903228261656479, 0.8869376750668644, 0.8166597939933369, 0.27539566443012947, 0.18359710962008632, 0.09179855481004316, 0.36719421924017265, 0.7236847519722771, 0.12061412532871287, 0.12061412532871287, 0.0989397337351885, 0.2143694230929084, 0.04946986686759425, 0.13191964498025133, 0.18138951184784557, 0.016489955622531416, 0.04946986686759425, 0.0989397337351885, 0.11542968935771991, 0.03297991124506283, 0.6357120748723565, 0.6970327567040876, 0.5072136780155828, 0.1127141506701295, 0.05635707533506475, 0.2817853766753238, 0.7245530490176205, 0.7304827650295042, 0.14609655300590083, 0.6533162013737814, 0.7945012521044529, 0.0293915974582749, 0.1469579872913745, 0.0881747923748247, 0.2351327796661992, 0.2057411822079243, 0.0881747923748247, 0.0293915974582749, 0.0587831949165498, 0.0881747923748247, 0.941153367091108, 0.7882664641634277, 0.35413921872430687, 0.5902320312071782, 0.1764812427810599, 0.7059249711242396, 0.6865333294180354, 0.7951889710631301, 0.17591725403629124, 0.5277517621088738, 0.17591725403629124, 0.7363418358709864, 0.11932052596388952, 0.7159231557833371, 0.10375597083898701, 0.051877985419493504, 0.1556339562584805, 0.1556339562584805, 0.051877985419493504, 0.41502388335594803, 0.12230679813339872, 0.48922719253359487, 0.12230679813339872, 0.12230679813339872, 0.14383551307659775, 0.8630130784595865, 0.10650799926270127, 0.053253999631350635, 0.10650799926270127, 0.4260319970508051, 0.053253999631350635, 0.21301599852540254, 0.10520846615580204, 0.21041693231160408, 0.36822963154530713, 0.21041693231160408, 0.05260423307790102, 0.8479730388492218, 0.9315733179006727, 0.7529010494104129, 0.9205463322507477, 0.05037090606603673, 0.2266690772971653, 0.12592726516509184, 0.0755563590990551, 0.2266690772971653, 0.025185453033018366, 0.10074181213207346, 0.10074181213207346, 0.05037090606603673, 0.42241965209487753, 0.07040327534914625, 0.21120982604743876, 0.2464114637220119, 0.03520163767457313, 0.058909459811012886, 0.11781891962202577, 0.058909459811012886, 0.3534567588660773, 0.17672837943303865, 0.23563783924405154, 0.16857315134797762, 0.6742926053919105, 0.1733938313515255, 0.24275136389213572, 0.1040362988109153, 0.06935753254061021, 0.27743013016244084, 0.1040362988109153, 0.7699301441030443, 0.05720804087660637, 0.18592613284897072, 0.2002281430681223, 0.10011407153406116, 0.05720804087660637, 0.10011407153406116, 0.10011407153406116, 0.07151005109575796, 0.11441608175321274, 0.1629080333978119, 0.8145401669890595, 0.11084344045707932, 0.22168688091415864, 0.11084344045707932, 0.5542172022853966, 0.08045993114718626, 0.16091986229437252, 0.16091986229437252, 0.08045993114718626, 0.5632195180303038, 0.8270678322306456, 0.09433553450178556, 0.12578071266904742, 0.22011624717083295, 0.09433553450178556, 0.4402324943416659, 0.21784545266284527, 0.5446136316571132, 0.21784545266284527, 0.2575160221694711, 0.08583867405649036, 0.6008707183954325, 0.6446580771026884, 0.6310527619910881, 0.23664478574665804, 0.07888159524888601, 0.7496651265152576, 0.12494418775254293, 0.12494418775254293, 0.3415222107197064, 0.02627093928613126, 0.10508375714452504, 0.07881281785839378, 0.02627093928613126, 0.05254187857226252, 0.23643845357518134, 0.1313546964306563, 0.07448363965041747, 0.4469018379025048, 0.4469018379025048, 0.6608315077324782, 0.2643326030929913, 0.2727356115492532, 0.31469493640298446, 0.10489831213432815, 0.0629389872805969, 0.020979662426865633, 0.041959324853731265, 0.020979662426865633, 0.020979662426865633, 0.14685763698805943, 0.06641891300499152, 0.8634458690648898, 0.7025993985798513, 0.8401869861696253, 0.7673859282383669, 0.2526002071785956, 0.15156012430715737, 0.6062404972286295, 0.3227064875149528, 0.0691513901817756, 0.2766055607271024, 0.1613532437574764, 0.0691513901817756, 0.0922018535757008, 0.0230504633939252, 0.7452242697847288, 0.8616238800148079, 0.8995824005237112, 0.7155449512379378, 0.8194122591101908, 0.09104580656779898, 0.2509374937712997, 0.2509374937712997, 0.2509374937712997, 0.12546874688564985, 0.06273437344282493, 0.03136718672141246, 0.8123243126453564, 0.13066197411012578, 0.1596979683568204, 0.10888497842510482, 0.07258998561673655, 0.11614397698677847, 0.0798489841784102, 0.09436698130175751, 0.07258998561673655, 0.13792097267179942, 0.021776995685020964, 0.230898796107856, 0.692696388323568, 0.8321396533042857, 0.20803491332607144, 0.7470572764917409, 0.12450954608195682, 0.12450954608195682, 0.8179442405803549, 0.8464001835855293, 0.7241981250148558, 0.19396984512925222, 0.7758793805170089, 0.783921827198814, 0.9525826453157257, 0.24568410796978837, 0.6142102699244709, 0.8475097606940107, 0.7315287977060874, 0.6816907989039825, 0.7624336980602627, 0.15248673961205253, 0.7525815235851486, 0.93149521478458, 0.12841614491860398, 0.7704968695116239, 0.12841614491860398, 0.1453537139880135, 0.4360611419640405, 0.290707427976027, 0.048451237996004495, 0.048451237996004495, 0.048451237996004495, 0.12512500650760208, 0.12512500650760208, 0.7507500390456124, 0.6324712699366211, 0.18070607712474887, 0.09035303856237444, 0.7478960225198835, 0.913507864005582, 0.8115239741002624, 0.8095278160907929, 0.6298113626504652, 0.8136613025816428, 0.10589514921892175, 0.3706330222662261, 0.0794213619141913, 0.10589514921892175, 0.0794213619141913, 0.13236893652365217, 0.10589514921892175, 0.16195249678958287, 0.06478099871583315, 0.22673349550541602, 0.06478099871583315, 0.1295619974316663, 0.04858574903687486, 0.11336674775270801, 0.04858574903687486, 0.11336674775270801, 0.016195249678958287, 0.9165591266174802, 0.7971582734248202, 0.8760010133547897, 0.8136614796049311, 0.8616188725012323, 0.12308841035731889, 0.026012670952650807, 0.4162027352424129, 0.026012670952650807, 0.3381647223844605, 0.052025341905301614, 0.026012670952650807, 0.07803801285795242, 0.25635440890602346, 0.1709029392706823, 0.598160287447388, 0.9531627382475385, 0.0996393007890343, 0.3985572031561372, 0.0996393007890343, 0.24909825197258573, 0.04981965039451715, 0.0996393007890343, 0.9464475222072114, 0.4945239573916731, 0.134870170197729, 0.134870170197729, 0.04495672339924301, 0.08991344679848602, 0.08991344679848602, 0.8471591191219874, 0.0643061552207891, 0.1929184656623673, 0.3858369313247346, 0.2572246208831564, 0.903394591285153, 0.8402404065932858, 0.3388002409981843, 0.06160004381785169, 0.030800021908925845, 0.06160004381785169, 0.12320008763570338, 0.15400010954462923, 0.12320008763570338, 0.09240006572677753, 0.233044084986493, 0.233044084986493, 0.1165220424932465, 0.2913051062331163, 0.1165220424932465, 0.07743245325178603, 0.2322973597553581, 0.07743245325178603, 0.4645947195107162, 0.15486490650357207, 0.08448481715545814, 0.5913937200882069, 0.2534544514663744, 0.12604301819382704, 0.8192796182598757, 0.19395990511262687, 0.7758396204505075, 0.38943784538855053, 0.11126795582530015, 0.11126795582530015, 0.38943784538855053, 0.09276849124724464, 0.6493794387307125, 0.18553698249448927, 0.09276849124724464, 0.8188747816859756, 0.729397703294978, 0.17292317204420724, 0.08646158602210362, 0.12969237903315542, 0.23776936156078493, 0.04323079301105181, 0.04323079301105181, 0.21615396505525902, 0.06484618951657771, 0.16439252153997153, 0.8219626076998576, 0.14244335247106413, 0.08546601148263848, 0.2563980344479154, 0.1139546819768513, 0.17093202296527696, 0.028488670494212824, 0.08546601148263848, 0.08546601148263848, 0.05697734098842565, 0.8961274690071781, 0.9289550414832987, 0.7056916488229341, 0.6999717367776196, 0.7319990206972026, 0.5235422580555247, 0.2959151893357313, 0.13657624123187598, 0.022762706871979333, 0.3327635290000345, 0.2911680878750302, 0.08319088225000862, 0.12478632337501293, 0.04159544112500431, 0.12478632337501293, 0.09724664335572608, 0.19449328671145216, 0.3889865734229043, 0.02431166083893152, 0.04862332167786304, 0.1215583041946576, 0.02431166083893152, 0.04862332167786304, 0.07293498251679456, 0.8053219327661608, 0.3282043065148142, 0.20512769157175886, 0.2871787682004624, 0.041025538314351774, 0.08205107662870355, 0.7207314682964365, 0.9460250260810646, 0.1127577217193663, 0.5637886085968314, 0.2255154434387326, 0.05212400731263366, 0.05212400731263366, 0.6776120950642376, 0.05212400731263366, 0.10424801462526732, 0.05212400731263366, 0.6989683186337101, 0.810340107114512, 0.7998127741592053, 0.1347090818394229, 0.20206362275913436, 0.20206362275913436, 0.06735454091971145, 0.40412724551826873, 0.6675493793969911, 0.13503811048601186, 0.18905335468041662, 0.10803048838880949, 0.5131448198468451, 0.6279404543545536, 0.9488196256392499, 0.8175933357119292, 0.10219916696399114, 0.10219916696399114, 0.8601274523526334, 0.1855744134707318, 0.10309689637262878, 0.06185813782357727, 0.22681317201978332, 0.10309689637262878, 0.10309689637262878, 0.04123875854905151, 0.12371627564715454, 0.06185813782357727, 0.8609062017268139, 0.121167568223257, 0.1696345955125598, 0.2181016228018626, 0.121167568223257, 0.2665686500911654, 0.0484670272893028, 0.0484670272893028, 0.0242335136446514, 0.7468622994962331, 0.8256677815794208, 0.9149977858289925, 0.6444339299393773, 0.20772818544569838, 0.04154563708913968, 0.20772818544569838, 0.2908194596239777, 0.08309127417827936, 0.1661825483565587, 0.1311630573141107, 0.6558152865705535, 0.1311630573141107, 0.7608132135186771, 0.1548771325282899, 0.7743856626414496, 0.21318367820518386, 0.6821877702565884, 0.042636735641036774, 0.042636735641036774, 0.9379701169861009, 0.8616764939554239, 0.5506335469908495, 0.15732387056881417, 0.23598580585322124, 0.10688759902191848, 0.10688759902191848, 0.10688759902191848, 0.6413255941315109, 0.10688759902191848, 0.8349731033662869, 0.894231575136069, 0.9510799107117013, 0.8174537309209968, 0.7796506537337988, 0.8535663953537275, 0.8862982778753591, 0.08302886442777463, 0.8302886442777463, 0.12925206203478493, 0.7755123722087096, 0.7492054487664046, 0.18730136219160115, 0.22187164236376256, 0.32048126119210146, 0.049304809414169454, 0.09860961882833891, 0.22187164236376256, 0.024652404707084727, 0.049304809414169454, 0.024652404707084727, 0.14269022125617364, 0.5231974779393033, 0.09512681417078242, 0.04756340708539121, 0.04756340708539121, 0.09512681417078242, 0.8743261267232955, 0.7249137883073135, 0.8242597473111368, 0.7312860948699732, 0.27053034810289145, 0.20289776107716856, 0.09017678270096381, 0.09017678270096381, 0.20289776107716856, 0.045088391350481906, 0.11272097837620476, 0.7801341299302806, 0.8227647376355007, 0.673340418535777, 0.22444680617859233, 0.8571666444989819, 0.6341644292697705, 0.6572223354360944, 0.7056874147904533, 0.24176017715097828, 0.08058672571699275, 0.3626402657264674, 0.08058672571699275, 0.1611734514339855, 0.6813244534737106, 0.9402514751552451, 0.6514634349105232, 0.3257317174552616, 0.09291301165714032, 0.18582602331428064, 0.6503910815999823, 0.09291301165714032, 0.7727747125201762, 0.12949918988799677, 0.5179967595519871, 0.25899837977599355, 0.8284151395585352, 0.24249350800567185, 0.1454961048034031, 0.09699740320226874, 0.43648831441020935, 0.04849870160113437, 0.04849870160113437, 0.8915677385582754, 0.8906870338829882, 0.7964730835406395, 0.8099901228274512, 0.7904868626040813, 0.13174781043401357, 0.8238627310284365, 0.2521647354643226, 0.05043294709286452, 0.15129884127859355, 0.3025976825571871, 0.15129884127859355, 0.10086589418572904, 0.6892787368855219, 0.03829326316030677, 0.15317305264122708, 0.11487978948092031, 0.072030264511184, 0.16807061719276264, 0.072030264511184, 0.12005044085197332, 0.2641109698743413, 0.024010088170394663, 0.048020176340789325, 0.16807061719276264, 0.072030264511184, 0.18478488586365022, 0.36956977172730043, 0.09239244293182511, 0.18478488586365022, 0.18478488586365022, 0.6314174222548522, 0.15785435556371305, 0.10523623704247537, 0.05261811852123768, 0.05261811852123768, 0.05261811852123768, 0.6774500727168262, 0.22581669090560874, 0.8583771710290733, 0.19839980318952313, 0.06613326772984104, 0.2645330709193642, 0.1322665354596821, 0.3306663386492052, 0.7531052937142004, 0.6881714751491926, 0.22939049171639755, 0.7196771000607507, 0.5750053024726132, 0.3450031814835679, 0.849274898493022, 0.12394719191110037, 0.12394719191110037, 0.3718415757333011, 0.06197359595555019, 0.24789438382220075, 0.1144515315639807, 0.17167729734597106, 0.2289030631279614, 0.5150318920379132, 0.7605471016326342, 0.755238640503597, 0.1258731067505995, 0.8527731655525482, 0.9280438095271711, 0.7001370154221233, 0.08432888907295866, 0.25298666721887597, 0.4216444453647933, 0.12649333360943799, 0.08432888907295866, 0.09915592560404851, 0.7932474048323881, 0.09915592560404851, 0.7805626842023283, 0.8714102716012307, 0.7913208774666293, 0.09891510968332866, 0.7128652358190856, 0.7398559452764606, 0.9449142196966992, 0.07314918908716732, 0.8046410799588405, 0.07314918908716732, 0.6032345477923508, 0.10053909129872513, 0.20107818259745025, 0.10053909129872513, 0.7899209919523528, 0.15798419839047054, 0.9200548118717154, 0.7278616579901446, 0.10398023685573494, 0.10398023685573494, 0.12237200487357763, 0.7342320292414658, 0.12237200487357763, 0.9093636012805363, 0.9519064530987533, 0.9007488389265057, 0.7063595317089639, 0.8867661836648895, 0.13179070806834126, 0.13179070806834126, 0.6589535403417063, 0.13179070806834126, 0.17704962235313068, 0.17704962235313068, 0.08852481117656534, 0.04426240558828267, 0.132787216764848, 0.04426240558828267, 0.04426240558828267, 0.265574433529696, 0.653995413894228, 0.7087307222168903, 0.8690511486681941, 0.8634771915591478, 0.8982000806420576, 0.11539982892436713, 0.46159931569746854, 0.28849957231091783, 0.11539982892436713, 0.6492974505326881, 0.08598482730704649, 0.25795448192113946, 0.08598482730704649, 0.3009468955746627, 0.04299241365352324, 0.17196965461409297, 0.21737848124863962, 0.32606772187295946, 0.10868924062431981, 0.1449189874990931, 0.10868924062431981, 0.07245949374954655, 0.036229746874773275, 0.783891964077666, 0.853650967454942, 0.858402518877647, 0.1283426067055882, 0.21390434450931364, 0.14973304115651956, 0.0641713033527941, 0.04278086890186273, 0.0641713033527941, 0.0641713033527941, 0.2566852134111764, 0.6390748888109175, 0.8171865891010448, 0.2179395529414833, 0.6538186588244499, 0.7126899005334144, 0.10415867879377214, 0.6770314121595189, 0.05207933939688607, 0.10415867879377214, 0.03348306155216177, 0.40179673862594123, 0.10044918465648531, 0.26786449241729415, 0.13393224620864708, 0.06696612310432354, 0.839820324934604, 0.6828244853218308, 0.694840620846917, 0.9270302789870182, 0.9169264688361821, 0.7529077268891182, 0.2565214147063268, 0.7054338904423987, 0.5878962632257474, 0.2939481316128737, 0.15253179667225117, 0.7626589833612558, 0.31605439508392147, 0.15802719754196073, 0.22123807655874503, 0.1264217580335686, 0.03160543950839215, 0.15802719754196073, 0.8701691820328733, 0.6806728574765358, 0.6867164890304206, 0.08583956112880257, 0.08583956112880257, 0.08583956112880257, 0.8558230126098559, 0.15692935806785563, 0.39232339516963904, 0.39232339516963904, 0.7957897964450023, 0.6420755511513001, 0.14268345581140002, 0.14268345581140002, 0.720935186897512, 0.25509459552619945, 0.5739628399339487, 0.12754729776309973, 0.08476289017117745, 0.39556015413216145, 0.08476289017117745, 0.028254296723725817, 0.3390515606847098, 0.056508593447451634, 0.11723445781737135, 0.058617228908685676, 0.11723445781737135, 0.17585168672605703, 0.35170337345211405, 0.11723445781737135, 0.11723445781737135, 0.8167000094248777, 0.6814684421246964, 0.2336281143931616, 0.0584070285982904, 0.6424773145811945, 0.5947128455737024, 0.06996621712631793, 0.06996621712631793, 0.13993243425263585, 0.10494932568947689, 0.8005949158876557, 0.9253243209531707, 0.07117879391947468, 0.6366227564410591, 0.7754703160754658, 0.19386757901886645, 0.9483553728552482, 0.1649642584830982, 0.10997617232206547, 0.1649642584830982, 0.054988086161032734, 0.4948927754492946, 0.3777832056581371, 0.025185547043875805, 0.15111328226325482, 0.025185547043875805, 0.025185547043875805, 0.05037109408775161, 0.15111328226325482, 0.05037109408775161, 0.07555664113162741, 0.26719846946792225, 0.623463095425152, 0.8138118671591092, 0.2726627093480653, 0.5453254186961306, 0.35478546821898826, 0.11826182273966275, 0.16894546105666106, 0.06757818442266443, 0.05068363831699832, 0.11826182273966275, 0.11826182273966275, 0.09287246720919072, 0.7429797376735258, 0.09287246720919072, 0.09287246720919072, 0.915974585259507, 0.12551627982558106, 0.031379069956395264, 0.06275813991279053, 0.2824116296075574, 0.12551627982558106, 0.09413720986918579, 0.031379069956395264, 0.06275813991279053, 0.1568953497819763, 0.36301643658418686, 0.29041314926734946, 0.2178098619505121, 0.03630164365841868, 0.07260328731683736, 0.9635824732318952, 0.18800299092598716, 0.06266766364199572, 0.2506706545679829, 0.3760059818519743, 0.06266766364199572, 0.9031810475506548, 0.8747830356280135, 0.14579717260466893, 0.9594480824183805, 0.9494753873051426, 0.20631611186501753, 0.14736865133215538, 0.20631611186501753, 0.3242110329307418, 0.02947373026643108, 0.05894746053286216, 0.3165092319692384, 0.144689934614509, 0.24416426466198393, 0.09043120913406812, 0.09043120913406812, 0.03617248365362725, 0.04521560456703406, 0.018086241826813624, 0.027129362740220438, 0.8322451789018291, 0.16644903578036585, 0.3546577724052125, 0.13066338983349934, 0.07466479419057105, 0.1493295883811421, 0.09333099273821382, 0.018666198547642763, 0.1493295883811421, 0.018666198547642763, 0.2616669464197842, 0.0670940888255857, 0.0670940888255857, 0.18786344871163993, 0.0670940888255857, 0.1073505421209371, 0.07380349770814426, 0.09393172435581996, 0.0670940888255857, 0.006709408882558569, 0.44195404201199107, 0.14731801400399702, 0.39284803734399204, 0.7123210582653653, 0.23744035275512176, 0.7501655894796546, 0.16670346432881214, 0.08335173216440607, 0.8262510759951976, 0.11803586799931395, 0.8439187168461337, 0.6566436318448099, 0.26265745273792396, 0.7940825989447511, 0.9165081240751195, 0.06092660709779673, 0.06092660709779673, 0.18277982129339018, 0.18277982129339018, 0.06092660709779673, 0.06092660709779673, 0.12185321419559346, 0.06092660709779673, 0.24370642839118692, 0.15580804550551786, 0.7790402275275893, 0.8779995157358212, 0.8767424565496277, 0.6845037737462498, 0.8224154852266334, 0.7841711142827196, 0.7530935864987687, 0.06311719447323373, 0.5680547502591036, 0.2524687778929349, 0.12623438894646746, 0.14589623554481967, 0.14589623554481967, 0.5835849421792787, 0.04534401874866102, 0.27206411249196616, 0.27206411249196616, 0.04534401874866102, 0.1813760749946441, 0.13603205624598308, 0.7656175329965005, 0.6226400835607815, 0.10377334726013024, 0.20754669452026048, 0.8222715163946492, 0.17132759544041412, 0.08566379772020706, 0.5996465840414494, 0.08566379772020706, 0.9293762543378496, 0.21046177159954663, 0.6313853147986398, 0.9205634929014748, 0.263327671528521, 0.11285471636936613, 0.4138006266876758, 0.18809119394894355, 0.17102461016977671, 0.13301914124315967, 0.09501367231654262, 0.0760109378532341, 0.34204922033955343, 0.03800546892661705, 0.05700820338992557, 0.05700820338992557, 0.019002734463308524, 0.1824877149642635, 0.729950859857054, 0.6378570929187755, 0.8830956535164386, 0.2824562222920978, 0.20175444449435556, 0.4035088889887111, 0.08070177779774222, 0.2705318385084345, 0.23188443300722958, 0.3864740550120493, 0.07729481100240986, 0.05100441782952573, 0.15301325348857717, 0.10200883565905146, 0.40803534263620583, 0.05100441782952573, 0.15301325348857717, 0.10200883565905146, 0.7593899483144705, 0.017667930850960398, 0.3886944787211287, 0.017667930850960398, 0.10600758510576239, 0.035335861701920795, 0.40636240957208913, 0.05300379255288119, 0.9036673496891083, 0.2981443705116709, 0.037268046313958865, 0.2981443705116709, 0.1118041389418766, 0.037268046313958865, 0.037268046313958865, 0.18634023156979435, 0.8009682536756105, 0.8552091017675928, 0.1425348502945988, 0.8825654685217742, 0.09059479959020886, 0.9059479959020885, 0.694091981933312, 0.11019948322970097, 0.47753109399537086, 0.03673316107656699, 0.03673316107656699, 0.03673316107656699, 0.07346632215313398, 0.14693264430626796, 0.07346632215313398, 0.03673316107656699, 0.04068370576043871, 0.2712247050695914, 0.29834717557655055, 0.10848988202783656, 0.08136741152087743, 0.04068370576043871, 0.01356123525347957, 0.06780617626739785, 0.06780617626739785, 0.12167633896572971, 0.12167633896572971, 0.7300580337943783, 0.6907721827500191, 0.7316396972425085, 0.7340210780185983, 0.11173697109857682, 0.16760545664786522, 0.12570409248589892, 0.32124379190840835, 0.11173697109857682, 0.041901364161966305, 0.041901364161966305, 0.013967121387322102, 0.05586848554928841, 0.5003831776964001, 0.27293627874349097, 0.13646813937174548, 0.09097875958116365, 0.6704751953751228, 0.0744972439305692, 0.1489944878611384, 0.0744972439305692, 0.0744972439305692], \"Term\": [\"abortion\", \"academy\", \"access\", \"access\", \"access\", \"accord\", \"accord\", \"accountability\", \"accountable\", \"acre\", \"acre\", \"act\", \"act\", \"act\", \"act\", \"act\", \"affect\", \"affect\", \"aggie\", \"agriculture\", \"air\", \"allow\", \"allow\", \"already\", \"aluminum\", \"amazing\", \"amazing\", \"amp\", \"anniversary\", \"announcement\", \"appearance\", \"appropriation\", \"approve\", \"army\", \"article\", \"assassination\", \"asset\", \"assist\", \"attention\", \"au\", \"authorization\", \"authorize\", \"award\", \"award\", \"award\", \"bank\", \"basketball\", \"bc\", \"benefit\", \"benefit\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"bill\", \"bill\", \"bill\", \"bill\", \"bipartisan\", \"bipartisan\", \"bipartisan\", \"bipartisan\", \"bipartisan\", \"blessing\", \"block\", \"blue\", \"bonus\", \"book\", \"book\", \"branch\", \"break\", \"break\", \"break\", \"brick\", \"brief\", \"bring\", \"bring\", \"bring\", \"budget\", \"budget\", \"budget\", \"budget\", \"budget\", \"build\", \"build\", \"build\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"ca\", \"campaign\", \"campaign\", \"campaign\", \"campaign\", \"candidate\", \"cap\", \"career\", \"cast\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebrate\", \"ceremony\", \"ceremony\", \"chairman\", \"check\", \"check\", \"check\", \"check\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"claim\", \"class\", \"class\", \"class\", \"clean\", \"coach\", \"coffee\", \"colleague\", \"colleague\", \"colleague\", \"colleague\", \"colleague\", \"colleague\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"commercial\", \"commit\", \"committee\", \"committee\", \"committee\", \"committee\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"company\", \"company\", \"complain\", \"complete\", \"congrat\", \"congrat\", \"congratulation\", \"congratulation\", \"congratulation\", \"congratulation\", \"congratulation\", \"congratulation\", \"congressman\", \"congressman\", \"congressman\", \"consider\", \"constituent\", \"constituent\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"control\", \"control\", \"conversation\", \"conversation\", \"corner\", \"corporation\", \"corporation\", \"corruption\", \"cosponsor\", \"cosponsor\", \"council\", \"counsel\", \"counsel\", \"counting\", \"country\", \"country\", \"country\", \"country\", \"country\", \"couple\", \"crime\", \"crime\", \"criminal\", \"critic\", \"crop\", \"crucial\", \"current\", \"current\", \"currently\", \"cut\", \"cut\", \"cut\", \"cut\", \"date\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deep\", \"democracy\", \"democracy\", \"deny\", \"deny\", \"department\", \"detail\", \"development\", \"directly\", \"director\", \"director\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"disposal\", \"dyess\", \"economy\", \"economy\", \"economy\", \"economy\", \"economy\", \"education\", \"educator\", \"educator\", \"effect\", \"effort\", \"effort\", \"effort\", \"effort\", \"effort\", \"effort\", \"elect_official\", \"elect_official\", \"election\", \"election\", \"election\", \"election\", \"election\", \"employ\", \"employee\", \"employee\", \"enforcement\", \"enjoy\", \"enjoy\", \"enjoy\", \"entire\", \"entrance\", \"evening\", \"ever\", \"ever\", \"ever\", \"ever\", \"everyday\", \"evidence\", \"exactly\", \"examine\", \"excellence\", \"exchange\", \"executive\", \"existence\", \"experience\", \"extend\", \"extra\", \"fact\", \"fact\", \"fact\", \"fact\", \"fall\", \"fall\", \"fall\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"farmer\", \"fed\", \"federal\", \"federal\", \"federal\", \"federal\", \"festival\", \"final\", \"final\", \"finally\", \"firefighter\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fix\", \"flight\", \"folk\", \"folk\", \"food\", \"food\", \"forecasting\", \"forest\", \"former\", \"former\", \"former\", \"found\", \"free\", \"free\", \"full\", \"full\", \"full\", \"full\", \"full\", \"full\", \"fully\", \"fully\", \"fully\", \"fully\", \"fun\", \"fun\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"fund\", \"funding\", \"funding\", \"funding\", \"funding\", \"funding\", \"game\", \"gavel\", \"generating\", \"genocide\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goal\", \"goal\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"graham\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"group\", \"group\", \"grow\", \"grow\", \"grow\", \"grow\", \"gun\", \"gun\", \"gun\", \"gun\", \"gun\", \"hall\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy_birthday\", \"happy_birthday\", \"happy_birthday\", \"hard\", \"hard\", \"hard\", \"harvey\", \"health_care\", \"health_care\", \"health_care\", \"healthcare\", \"healthcare\", \"healthcare\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hearing\", \"hearing\", \"hearing\", \"heart\", \"heart\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"here\", \"here\", \"heritage\", \"highlight\", \"historic\", \"hold\", \"hold\", \"hold\", \"honor\", \"honor\", \"honor\", \"honor\", \"honor\", \"honor\", \"honor\", \"hoosi\", \"hoosier\", \"hostage\", \"hot\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"httpst\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstco\", \"httpstcob\", \"httpstcob\", \"httpstcoc\", \"httpstcoc\", \"httpstcoi\", \"httpstcoi\", \"httpstcoi\", \"httpstcos\", \"httpstcov\", \"httpstcovg\", \"httpstcow\", \"httpstcow\", \"httptco\", \"human\", \"hurricane\", \"hurricane\", \"icymi\", \"idea\", \"identify\", \"immigration\", \"immigration\", \"immoral\", \"impact\", \"importance\", \"importance\", \"importance\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"improve\", \"improve\", \"improve\", \"incredible\", \"incredible\", \"incredible\", \"indy\", \"innovation\", \"institution\", \"invest\", \"invite\", \"jake\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"job\", \"join\", \"join\", \"join\", \"join\", \"join\", \"join\", \"join\", \"join\", \"join\", \"join\", \"keynote\", \"king\", \"korean\", \"laird\", \"large\", \"large\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last_night\", \"last_night\", \"last_night\", \"law_enforcement\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leg\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"less\", \"life\", \"life\", \"life\", \"life\", \"limited\", \"line\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"local\", \"local\", \"local\", \"local\", \"local\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look_forward\", \"look_forward\", \"look_forward\", \"lose\", \"lose\", \"loss\", \"loss\", \"love\", \"love\", \"love\", \"love\", \"low\", \"low\", \"low\", \"low\", \"luck\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man_woman\", \"man_woman\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"mark\", \"market\", \"material\", \"me\", \"meaningful\", \"meet\", \"meet\", \"meet\", \"meet\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"mental\", \"military\", \"military\", \"military\", \"military\", \"military\", \"min\", \"mom\", \"money\", \"money\", \"money\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"monument\", \"more\", \"motorcycle\", \"much\", \"much\", \"much\", \"much\", \"much\", \"music\", \"national\", \"national\", \"national\", \"national\", \"nature\", \"nd\", \"nearly\", \"nearly\", \"nearly\", \"necessary\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neighborhood\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newsletter\", \"nomination\", \"north\", \"nuclear\", \"office\", \"office\", \"office\", \"office\", \"office\", \"office\", \"officer\", \"officer\", \"officer\", \"old\", \"online\", \"online\", \"open\", \"open\", \"open\", \"open\", \"operate\", \"operation\", \"opioid\", \"opioid\", \"opioid\", \"opportunity\", \"opportunity\", \"opportunity\", \"opportunity\", \"opportunity\", \"orange\", \"order\", \"organization\", \"outdated\", \"overdose\", \"package\", \"parent\", \"park\", \"park\", \"participate\", \"participate\", \"particularly\", \"particularly\", \"pass\", \"pass\", \"pass\", \"pass\", \"pass\", \"pass\", \"pass\", \"pass\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"paycheck\", \"payment\", \"peace\", \"pen\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"perfect\", \"person\", \"person\", \"personal\", \"pill\", \"pioneer\", \"pipeline\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plane\", \"play\", \"pm\", \"pm\", \"power\", \"power\", \"power\", \"power\", \"practice\", \"pre\", \"pre\", \"pre\", \"premium\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"priority\", \"prisoner\", \"privilege\", \"pro\", \"process\", \"process\", \"produce\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"protect\", \"protect\", \"protect\", \"protect\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"push\", \"push\", \"quality\", \"question\", \"question\", \"question\", \"question\", \"question\", \"racist\", \"rank\", \"rank\", \"rapist\", \"rate\", \"rate\", \"rd\", \"re\", \"re\", \"re\", \"re\", \"re\", \"read\", \"read\", \"read\", \"read\", \"readiness\", \"real\", \"real\", \"reason\", \"reauthorization\", \"recall\", \"receive\", \"receive\", \"receive\", \"receive\", \"receive\", \"recognize\", \"recognize\", \"recognize\", \"redeem\", \"reduce\", \"reform\", \"reform\", \"reg\", \"regard\", \"regime\", \"release\", \"release\", \"release\", \"remember\", \"remember\", \"remember\", \"remember\", \"reminder\", \"reminder\", \"reopen\", \"research\", \"research\", \"research\", \"resident\", \"resident\", \"resident\", \"resilient\", \"responder\", \"response\", \"responsibility\", \"restore\", \"result\", \"result\", \"result\", \"result\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"rm\", \"robotic\", \"room\", \"rule\", \"rural\", \"safe\", \"safe\", \"safe\", \"safe\", \"saving\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"se\", \"secret\", \"sector\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"self\", \"sell\", \"senator\", \"senator\", \"senseless\", \"serve\", \"serve\", \"serve\", \"serve\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sex\", \"shift\", \"short\", \"signing\", \"sit\", \"skill\", \"small_business\", \"small_business\", \"social\", \"social\", \"space\", \"space\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"speaker\", \"specific\", \"spend\", \"spend\", \"spend\", \"spend\", \"sponsor\", \"st\", \"st\", \"st\", \"stable\", \"staff\", \"staff\", \"staff\", \"stagnant\", \"start\", \"start\", \"start\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"stay\", \"steel\", \"step\", \"step\", \"step\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"storm\", \"story\", \"story\", \"strategic\", \"strengthen\", \"strengthen\", \"strike\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"subcommittee\", \"subcommittee\", \"subsidy\", \"summit\", \"summit\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"sure\", \"sure\", \"sure\", \"sure\", \"survivor\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tax\", \"tax\", \"tax\", \"tax\", \"tax\", \"tax_code\", \"tax_cut\", \"tax_cut\", \"tax_cut\", \"tax_cut\", \"tax_cut\", \"tax_reform\", \"taxpayer\", \"taxpayer\", \"team\", \"test\", \"th\", \"th\", \"th\", \"th\", \"th\", \"th\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"threaten\", \"threaten\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"tomorrow\", \"tomorrow\", \"tomorrow\", \"tonight\", \"tonight\", \"tour\", \"tour\", \"tour\", \"trafficking\", \"trafficking\", \"train\", \"travel\", \"travel\", \"treasure\", \"treatment\", \"trump\", \"trump\", \"trump\", \"trump\", \"trump\", \"trump\", \"trump\", \"trump\", \"trump\", \"trust\", \"trust\", \"turn\", \"un\", \"understand\", \"unemployment\", \"uniform\", \"unique\", \"update\", \"update\", \"update\", \"update\", \"urge\", \"urge\", \"urge\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"value\", \"ve\", \"ve\", \"ve\", \"vet\", \"veteran\", \"veteran\", \"veteran\", \"veteran\", \"victim\", \"victory\", \"victory\", \"view\", \"visit\", \"visit\", \"visit\", \"visit\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"voting\", \"voting\", \"wa\", \"walk\", \"want\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"watch\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wear\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"weekly\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whole\", \"winner\", \"winner\", \"winter\", \"wish\", \"wish\", \"withhold\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"worship\", \"worth\", \"wrap\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yesterday\", \"yesterday\", \"yesterday\", \"yesterday\", \"young\", \"young\", \"young\", \"young\", \"young\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 9, 7, 4, 10, 5, 3, 1, 6, 8]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el904131402245504593603175286789\", ldavis_el904131402245504593603175286789_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el904131402245504593603175286789\", ldavis_el904131402245504593603175286789_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el904131402245504593603175286789\", ldavis_el904131402245504593603175286789_data);\n            })\n         });\n}\n</script>"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "          Party           Handle  \\\n3043   Democrat  CongressmanRaja   \n1058   Democrat      RepBarragan   \n32345  Democrat  BennieGThompson   \n25253  Democrat   RepAndreCarson   \n21433  Democrat    SanfordBishop   \n\n                                                   Tweet       retweeted  \\\n3043   Proud to have introduced the Help Students Vot...              []   \n1058   RT @GreenForAll: Thank you @RepMcEachin, @RepJ...  [@GreenForAll]   \n32345  RT @OfficialCBC: “I marvel at some of my colle...  [@OfficialCBC]   \n25253  My annual Youth Opportunities Fair is TODAY, M...              []   \n21433  With 755 farms and more than 423,000 acres ded...              []   \n\n                                       mentioned hashtags  \\\n3043                                [@SenBooker]       []   \n1058   [@RepMcEachin, @RepJayapal, @RepBarragan]       []   \n32345                                         []       []   \n25253                                         []       []   \n21433                                         []       []   \n\n                                            TweetCleaned  \\\n3043   proud introduce help student vote act exactly ...   \n1058   thank leadership fight environmental justice call   \n32345  marvel colleague probably hit gun want defend ...   \n25253  annual youth opportunity fair today march hope...   \n21433  farm acre dedicate cotton second congressional...   \n\n                                              TweetWords  \\\n3043   [proud, introduce, help, student, vote, act, e...   \n1058   [thank, leadership, fight, environmental, just...   \n32345  [marvel, colleague, probably, hit, gun, want, ...   \n25253  [annual, youth, opportunity, fair, today, marc...   \n21433  [farm, acre, dedicate, cotton, second, congres...   \n\n                                                bigramms  \\\n3043   [proud, introduce, help, student, vote, act, e...   \n1058   [thank, leadership, fight, environmental, just...   \n32345  [marvel, colleague, probably, hit, gun, want, ...   \n25253  [annual, youth, opportunity, fair, today, marc...   \n21433  [farm, acre, dedicate, cotton, second, congres...   \n\n                                                trigrams  \\\n3043   [proud, introduce, help, student, vote, act, e...   \n1058   [thank, leadership, fight, environmental, just...   \n32345  [marvel, colleague, probably, hit, gun, want, ...   \n25253  [annual, youth, opportunity, fair, today, marc...   \n21433  [farm, acre, dedicate, cotton, second, congres...   \n\n                                               topic_vec  len  \n3043   [0.0100015, 0.90997654, 0.010001422, 0.0100017...    5  \n1058   [0.014291703, 0.014294594, 0.014288709, 0.0142...    5  \n32345  [0.011114913, 0.011116227, 0.011115268, 0.0111...    5  \n25253  [0.007146218, 0.0071476027, 0.0071462737, 0.00...    5  \n21433  [0.009095226, 0.00909409, 0.009095232, 0.00909...    5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Party</th>\n      <th>Handle</th>\n      <th>Tweet</th>\n      <th>retweeted</th>\n      <th>mentioned</th>\n      <th>hashtags</th>\n      <th>TweetCleaned</th>\n      <th>TweetWords</th>\n      <th>bigramms</th>\n      <th>trigrams</th>\n      <th>topic_vec</th>\n      <th>len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3043</th>\n      <td>Democrat</td>\n      <td>CongressmanRaja</td>\n      <td>Proud to have introduced the Help Students Vot...</td>\n      <td>[]</td>\n      <td>[@SenBooker]</td>\n      <td>[]</td>\n      <td>proud introduce help student vote act exactly ...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n      <td>[proud, introduce, help, student, vote, act, e...</td>\n      <td>[0.0100015, 0.90997654, 0.010001422, 0.0100017...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1058</th>\n      <td>Democrat</td>\n      <td>RepBarragan</td>\n      <td>RT @GreenForAll: Thank you @RepMcEachin, @RepJ...</td>\n      <td>[@GreenForAll]</td>\n      <td>[@RepMcEachin, @RepJayapal, @RepBarragan]</td>\n      <td>[]</td>\n      <td>thank leadership fight environmental justice call</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n      <td>[thank, leadership, fight, environmental, just...</td>\n      <td>[0.014291703, 0.014294594, 0.014288709, 0.0142...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>32345</th>\n      <td>Democrat</td>\n      <td>BennieGThompson</td>\n      <td>RT @OfficialCBC: “I marvel at some of my colle...</td>\n      <td>[@OfficialCBC]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>marvel colleague probably hit gun want defend ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n      <td>[marvel, colleague, probably, hit, gun, want, ...</td>\n      <td>[0.011114913, 0.011116227, 0.011115268, 0.0111...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>25253</th>\n      <td>Democrat</td>\n      <td>RepAndreCarson</td>\n      <td>My annual Youth Opportunities Fair is TODAY, M...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>annual youth opportunity fair today march hope...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n      <td>[annual, youth, opportunity, fair, today, marc...</td>\n      <td>[0.007146218, 0.0071476027, 0.0071462737, 0.00...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21433</th>\n      <td>Democrat</td>\n      <td>SanfordBishop</td>\n      <td>With 755 farms and more than 423,000 acres ded...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>farm acre dedicate cotton second congressional...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n      <td>[farm, acre, dedicate, cotton, second, congres...</td>\n      <td>[0.009095226, 0.00909409, 0.009095232, 0.00909...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_document_topic(model, tri):\n",
    "    doc_bow = id2word.doc2bow(tri)\n",
    "    vec = model.get_document_topics(bow=doc_bow, minimum_probability=0)\n",
    "    return [v for _, v in vec]\n",
    "\n",
    "df['topic_vec'] = df['trigrams'].apply(lambda trigram: get_document_topic(lda_model, trigram))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def evaluate_model(model):\n",
    "    df['topic_vec'] = df['trigrams'].apply(lambda trigram: get_document_topic(model, trigram))\n",
    "    df['len'] = df['topic_vec'].apply(lambda x: len(x))\n",
    "\n",
    "    text_train, text_test, label_train, label_test = train_test_split(df[\"topic_vec\"], df['Party'], train_size=0.70, random_state=101, test_size=0.30, shuffle=True)\n",
    "    cl = MLPClassifier()\n",
    "    # Train and predict\n",
    "    cl.fit(text_train.to_list(), label_train)\n",
    "\n",
    "    prediction = cl.predict(text_test.to_list())\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion = confusion_matrix(label_test, prediction)\n",
    "    # increase size of plot and fontsize for improved visibility\n",
    "    #fig, ax_cm = plt.subplots(figsize=(10, 10))\n",
    "    #plt.rcParams.update({'font.size': 14})\n",
    "    # create plot for multi-class confusion matrix, use prepared axes object and change colormap for better contrast\n",
    "    #plot_confusion_matrix(cl, text_test.to_list(), label_test.to_list(), ax=ax_cm, cmap=plt.cm.Reds_r)\n",
    "    # show plot corresponding to printed name and accuracy\n",
    "    #plt.show()\n",
    "    accuracy = accuracy_score(label_test, prediction)\n",
    "    precision, recall, fscore, *rest = precision_recall_fscore_support(label_test, prediction, average='macro')\n",
    "    return confusion, accuracy, precision, recall, fscore\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the model see: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#5preparestopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "# gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=20,\n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='auto',\n",
    "#                                            per_word_topics=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    perplexitiy_values = []\n",
    "    model_list = []\n",
    "    evaluations = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(\"Generated model with \" + str(num_topics) + \" topics\")\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        perplexity = model.log_perplexity(corpus)\n",
    "        print('Perplexity: ', perplexity)  # a measure of how good the model is. lower the better.\n",
    "        #coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        #coherence_values.append(coherencemodel.get_coherence())\n",
    "        evaluation = evaluate_model(model)\n",
    "        evaluations.append(evaluation)\n",
    "        perplexitiy_values.append(perplexity)\n",
    "        #print(\"Coherence: \" + str(coherencemodel.get_coherence()))\n",
    "        print(\"Evaluation: \" + str(evaluation))\n",
    "\n",
    "    return model_list, perplexitiy_values, evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "\n",
    "limit=100\n",
    "start=2\n",
    "step=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated model with 2 topics\n",
      "Perplexity:  -8.153033454096482\n",
      "Evaluation: (array([[153, 136],\n",
      "       [172, 139]]), 0.4866666666666667, 0.4881118881118881, 0.48817855116323056, 0.4863870329401563)\n",
      "Generated model with 3 topics\n",
      "Perplexity:  -8.246725689954616\n",
      "Evaluation: (array([[104, 185],\n",
      "       [115, 196]]), 0.5, 0.4946607701434581, 0.49504333604067696, 0.48796140366847207)\n",
      "Generated model with 4 topics\n",
      "Perplexity:  -8.315422333498836\n",
      "Evaluation: (array([[130, 159],\n",
      "       [151, 160]]), 0.48333333333333334, 0.4821004250382088, 0.48214822149779146, 0.48203842940685043)\n",
      "Generated model with 5 topics\n",
      "Perplexity:  -8.372524769367441\n",
      "Evaluation: (array([[106, 183],\n",
      "       [125, 186]]), 0.4866666666666667, 0.4814697497624327, 0.48242637323512727, 0.4773755656108597)\n",
      "Generated model with 6 topics\n",
      "Perplexity:  -8.435106572718862\n",
      "Evaluation: (array([[187, 102],\n",
      "       [181, 130]]), 0.5283333333333333, 0.5342485007496252, 0.5325326271987895, 0.5240377742459026)\n",
      "Generated model with 7 topics\n",
      "Perplexity:  -8.490065890662592\n",
      "Evaluation: (array([[213,  76],\n",
      "       [214,  97]]), 0.5166666666666667, 0.5297613407155717, 0.524460663781306, 0.49789925666004897)\n",
      "Generated model with 8 topics\n",
      "Perplexity:  -8.55576486873746\n",
      "Evaluation: (array([[163, 126],\n",
      "       [152, 159]]), 0.5366666666666666, 0.5376775271512113, 0.5376339300615272, 0.5366460731588071)\n",
      "Generated model with 9 topics\n",
      "Perplexity:  -8.60742065315327\n",
      "Evaluation: (array([[217,  72],\n",
      "       [226,  85]]), 0.5033333333333333, 0.5156216301706661, 0.5120884745046117, 0.4780720190556256)\n",
      "Generated model with 10 topics\n",
      "Perplexity:  -8.657522746230967\n",
      "Evaluation: (array([[193,  96],\n",
      "       [214,  97]]), 0.48333333333333334, 0.48839607388830186, 0.4898585876567385, 0.46975916803503015)\n",
      "Generated model with 11 topics\n",
      "Perplexity:  -8.696291786162018\n",
      "Evaluation: (array([[146, 143],\n",
      "       [147, 164]]), 0.5166666666666667, 0.516247734877878, 0.5162607505646479, 0.5162312748139992)\n",
      "Generated model with 12 topics\n",
      "Perplexity:  -8.74293643076518\n",
      "Evaluation: (array([[175, 114],\n",
      "       [176, 135]]), 0.5166666666666667, 0.5203720866371468, 0.5198099667330522, 0.5145089285714286)\n",
      "Generated model with 13 topics\n",
      "Perplexity:  -8.765936318064046\n",
      "Evaluation: (array([[142, 147],\n",
      "       [166, 145]]), 0.47833333333333333, 0.4788071517523572, 0.4787937115455223, 0.4783202913406168)\n",
      "Generated model with 14 topics\n",
      "Perplexity:  -8.798877856907323\n",
      "Evaluation: (array([[129, 160],\n",
      "       [134, 177]]), 0.51, 0.507858424253365, 0.507749307402174, 0.5068438003220612)\n",
      "Generated model with 15 topics\n",
      "Perplexity:  -8.845024882034492\n",
      "Evaluation: (array([[124, 165],\n",
      "       [144, 167]]), 0.485, 0.4828493076784751, 0.48302161795302573, 0.4823412471276082)\n",
      "Generated model with 16 topics\n",
      "Perplexity:  -8.857981301624555\n",
      "Evaluation: (array([[143, 146],\n",
      "       [152, 159]]), 0.5033333333333333, 0.5030286190608502, 0.5030318539369597, 0.5029798968155132)\n",
      "Generated model with 17 topics\n",
      "Perplexity:  -8.887049064674637\n",
      "Evaluation: (array([[149, 140],\n",
      "       [144, 167]]), 0.5266666666666666, 0.5262531822881347, 0.526274213108735, 0.5262402829212959)\n",
      "Generated model with 18 topics\n",
      "Perplexity:  -8.922007650398596\n",
      "Evaluation: (array([[165, 124],\n",
      "       [192, 119]]), 0.47333333333333333, 0.4759484040529792, 0.4767854560019582, 0.47021940069966806)\n",
      "Generated model with 19 topics\n",
      "Perplexity:  -8.962582627562476\n",
      "Evaluation: (array([[165, 124],\n",
      "       [164, 147]]), 0.52, 0.5219775905965747, 0.5218015331723762, 0.5195676108497648)\n",
      "Generated model with 20 topics\n",
      "Perplexity:  -8.976695367252667\n",
      "Evaluation: (array([[180, 109],\n",
      "       [196, 115]]), 0.49166666666666664, 0.4960581306990881, 0.49630614492818126, 0.4856299627573607)\n",
      "Generated model with 21 topics\n",
      "Perplexity:  -9.00935109176404\n",
      "Evaluation: (array([[191,  98],\n",
      "       [213,  98]]), 0.4816666666666667, 0.4863861386138614, 0.48800609708608245, 0.46890716121485354)\n",
      "Generated model with 22 topics\n",
      "Perplexity:  -9.023031816029672\n",
      "Evaluation: (array([[184, 105],\n",
      "       [213,  98]]), 0.47, 0.47311734560931124, 0.47589537044248376, 0.45888305294324505)\n",
      "Generated model with 23 topics\n",
      "Perplexity:  -9.061801021843179\n",
      "Evaluation: (array([[169, 120],\n",
      "       [179, 132]]), 0.5016666666666667, 0.5047208538587848, 0.5046061927702801, 0.499764381774024)\n",
      "Generated model with 24 topics\n",
      "Perplexity:  -9.092310329137575\n",
      "Evaluation: (array([[148, 141],\n",
      "       [148, 163]]), 0.5183333333333333, 0.518092105263158, 0.5181132411353041, 0.5180321033979571)\n",
      "Generated model with 25 topics\n",
      "Perplexity:  -9.09697580670486\n",
      "Evaluation: (array([[186, 103],\n",
      "       [186, 125]]), 0.5183333333333333, 0.5241228070175439, 0.5227639381835578, 0.5133027767564184)\n",
      "Generated model with 26 topics\n",
      "Perplexity:  -9.122307944947703\n",
      "Evaluation: (array([[178, 111],\n",
      "       [196, 115]]), 0.48833333333333334, 0.49239269319956463, 0.49284593731572446, 0.4826293216040176)\n",
      "Generated model with 27 topics\n",
      "Perplexity:  -9.126637591255374\n",
      "Evaluation: (array([[126, 163],\n",
      "       [165, 146]]), 0.4533333333333333, 0.45274080005338135, 0.452719767687669, 0.4527252502780868)\n",
      "Generated model with 28 topics\n",
      "Perplexity:  -9.162309262835446\n",
      "Evaluation: (array([[162, 127],\n",
      "       [167, 144]]), 0.51, 0.5118832647293039, 0.5117880706282891, 0.5095586027424683)\n",
      "Generated model with 29 topics\n",
      "Perplexity:  -9.181022877483363\n",
      "Evaluation: (array([[157, 132],\n",
      "       [172, 139]]), 0.49333333333333335, 0.49505938828385243, 0.49509896638814405, 0.4928769225636407)\n",
      "Generated model with 30 topics\n",
      "Perplexity:  -9.215394305043546\n",
      "Evaluation: (array([[138, 151],\n",
      "       [159, 152]]), 0.48333333333333334, 0.4831483148314831, 0.4831273156132133, 0.483051883803404)\n",
      "Generated model with 31 topics\n",
      "Perplexity:  -9.227669161598268\n",
      "Evaluation: (array([[156, 133],\n",
      "       [187, 124]]), 0.4666666666666667, 0.4686503840001815, 0.4692531069549061, 0.4651453021929043)\n",
      "Generated model with 32 topics\n",
      "Perplexity:  -9.253674413662354\n",
      "Evaluation: (array([[154, 135],\n",
      "       [163, 148]]), 0.5033333333333333, 0.5043863071418221, 0.5043781083456647, 0.5032836616995033)\n",
      "Generated model with 33 topics\n",
      "Perplexity:  -9.254500193746892\n",
      "Evaluation: (array([[156, 133],\n",
      "       [166, 145]]), 0.5016666666666667, 0.5030273917511954, 0.5030151648327196, 0.5014991149803127)\n",
      "Generated model with 34 topics\n",
      "Perplexity:  -9.288417378534271\n",
      "Evaluation: (array([[146, 143],\n",
      "       [175, 136]]), 0.47, 0.4711419287843768, 0.47124467339423004, 0.46985273687135315)\n",
      "Generated model with 35 topics\n",
      "Perplexity:  -9.32723339720586\n",
      "Evaluation: (array([[157, 132],\n",
      "       [167, 144]]), 0.5016666666666667, 0.5031535158346753, 0.5031375515971472, 0.501432616978526)\n",
      "Generated model with 36 topics\n",
      "Perplexity:  -9.306895467336975\n",
      "Evaluation: (array([[155, 134],\n",
      "       [160, 151]]), 0.51, 0.5109440267335004, 0.510931363277295, 0.5099782212542779)\n",
      "Generated model with 37 topics\n",
      "Perplexity:  -9.35809495745429\n",
      "Evaluation: (array([[159, 130],\n",
      "       [170, 141]]), 0.5, 0.5017889388620329, 0.501774608084202, 0.4995495946351717)\n",
      "Generated model with 38 topics\n",
      "Perplexity:  -9.389940785987656\n",
      "Evaluation: (array([[141, 148],\n",
      "       [143, 168]]), 0.515, 0.5140622214298449, 0.5140410997007088, 0.5140158821613767)\n",
      "Generated model with 39 topics\n",
      "Perplexity:  -9.417497012876286\n",
      "Evaluation: (array([[145, 144],\n",
      "       [161, 150]]), 0.49166666666666664, 0.4920301453914899, 0.49202260817321064, 0.49163136328911733)\n",
      "Generated model with 40 topics\n",
      "Perplexity:  -9.413259340140261\n",
      "Evaluation: (array([[175, 114],\n",
      "       [203, 108]]), 0.4716666666666667, 0.4747247247247247, 0.47640160660443487, 0.46499545724323577)\n",
      "Generated model with 41 topics\n",
      "Perplexity:  -9.448419534349686\n",
      "Evaluation: (array([[164, 125],\n",
      "       [164, 147]]), 0.5183333333333333, 0.5202205882352942, 0.5200714293661478, 0.5179463513765217)\n",
      "Generated model with 42 topics\n",
      "Perplexity:  -9.446129330445112\n",
      "Evaluation: (array([[163, 126],\n",
      "       [185, 126]]), 0.4816666666666667, 0.4841954022988506, 0.4845792676821059, 0.479688035892045)\n",
      "Generated model with 43 topics\n",
      "Perplexity:  -9.487325651313165\n",
      "Evaluation: (array([[125, 164],\n",
      "       [134, 177]]), 0.5033333333333333, 0.5008435331015976, 0.5008288921772606, 0.4995745824190587)\n",
      "Generated model with 44 topics\n",
      "Perplexity:  -9.502328916079106\n",
      "Evaluation: (array([[158, 131],\n",
      "       [163, 148]]), 0.51, 0.5113388939135095, 0.5112985235705783, 0.5098638510697416)\n",
      "Generated model with 45 topics\n",
      "Perplexity:  -9.51576285459598\n",
      "Evaluation: (array([[162, 127],\n",
      "       [164, 147]]), 0.515, 0.5167144328511935, 0.5166112217536911, 0.5146966854283928)\n",
      "Generated model with 46 topics\n",
      "Perplexity:  -9.557877929387422\n",
      "Evaluation: (array([[152, 137],\n",
      "       [185, 126]]), 0.4633333333333333, 0.4650630140695693, 0.46554812581359384, 0.4623236967193953)\n",
      "Generated model with 47 topics\n",
      "Perplexity:  -9.590603766942301\n",
      "Evaluation: (array([[136, 153],\n",
      "       [136, 175]]), 0.5183333333333333, 0.5167682926829269, 0.5166445999621714, 0.5162896571347276)\n",
      "Generated model with 48 topics\n",
      "Perplexity:  -9.64156382752986\n",
      "Evaluation: (array([[152, 137],\n",
      "       [160, 151]]), 0.505, 0.5057425213675214, 0.5057410518586098, 0.5049986249961805)\n",
      "Generated model with 49 topics\n",
      "Perplexity:  -9.688598481541195\n",
      "Evaluation: (array([[176, 113],\n",
      "       [180, 131]]), 0.5116666666666667, 0.5156336341867747, 0.515109202372078, 0.5089042530902996)\n",
      "Generated model with 50 topics\n",
      "Perplexity:  -9.703361995445476\n",
      "Evaluation: (array([[149, 140],\n",
      "       [151, 160]]), 0.515, 0.515, 0.5150201938161305, 0.5148369313019097)\n",
      "Generated model with 51 topics\n",
      "Perplexity:  -9.657220493303612\n",
      "Evaluation: (array([[155, 134],\n",
      "       [179, 132]]), 0.47833333333333333, 0.4801562288955923, 0.4803847394830828, 0.4775656450728988)\n",
      "Generated model with 52 topics\n",
      "Perplexity:  -9.737749713997806\n",
      "Evaluation: (array([[170, 119],\n",
      "       [175, 136]]), 0.51, 0.5130434782608696, 0.512767164743711, 0.5084214867741972)\n",
      "Generated model with 53 topics\n",
      "Perplexity:  -9.752916955009761\n",
      "Evaluation: (array([[178, 111],\n",
      "       [180, 131]]), 0.515, 0.5192645089801007, 0.5185694099845348, 0.5120056122149522)\n",
      "Generated model with 54 topics\n",
      "Perplexity:  -9.8051310729624\n",
      "Evaluation: (array([[159, 130],\n",
      "       [166, 145]]), 0.5066666666666667, 0.5082517482517482, 0.5082054762514047, 0.5063979277606697)\n",
      "Generated model with 55 topics\n",
      "Perplexity:  -9.823438731526492\n",
      "Evaluation: (array([[156, 133],\n",
      "       [183, 128]]), 0.47333333333333333, 0.4752992235445699, 0.47568397512210864, 0.47218386708832566)\n",
      "Generated model with 56 topics\n",
      "Perplexity:  -9.868034183553398\n",
      "Evaluation: (array([[166, 123],\n",
      "       [173, 138]]), 0.5066666666666667, 0.5092055742040484, 0.5090621836023987, 0.5055899514498241)\n",
      "Generated model with 57 topics\n",
      "Perplexity:  -9.890806992811243\n",
      "Evaluation: (array([[148, 141],\n",
      "       [162, 149]]), 0.495, 0.4956062291434928, 0.4956052025500951, 0.4949985972183256)\n",
      "Generated model with 58 topics\n",
      "Perplexity:  -9.896433779350684\n",
      "Evaluation: (array([[141, 148],\n",
      "       [161, 150]]), 0.485, 0.48512156095826486, 0.48510219294829715, 0.4848840989222575)\n",
      "Generated model with 59 topics\n",
      "Perplexity:  -9.97133888840992\n",
      "Evaluation: (array([[156, 133],\n",
      "       [163, 148]]), 0.5066666666666667, 0.5078593023126095, 0.5078383159581215, 0.506578947368421)\n",
      "Generated model with 60 topics\n",
      "Perplexity:  -9.98738155987062\n",
      "Evaluation: (array([[163, 126],\n",
      "       [165, 146]]), 0.515, 0.516857962697274, 0.5167336085181188, 0.5146103399673626)\n",
      "Generated model with 61 topics\n",
      "Perplexity:  -9.997898386108098\n",
      "Evaluation: (array([[145, 144],\n",
      "       [163, 148]]), 0.48833333333333334, 0.48881426792385696, 0.4888071740896094, 0.488320541346867)\n",
      "Generated model with 62 topics\n",
      "Perplexity:  -9.995783668292287\n",
      "Evaluation: (array([[171, 118],\n",
      "       [170, 141]]), 0.52, 0.5229339100306842, 0.5225358537589426, 0.518796992481203)\n",
      "Generated model with 63 topics\n",
      "Perplexity:  -10.080710221205134\n",
      "Evaluation: (array([[131, 158],\n",
      "       [161, 150]]), 0.4683333333333333, 0.4678215619996442, 0.4678011548860134, 0.46779965465369444)\n",
      "Generated model with 64 topics\n",
      "Perplexity:  -10.10831275016487\n",
      "Evaluation: (array([[154, 135],\n",
      "       [161, 150]]), 0.5066666666666667, 0.5076023391812865, 0.507593542429266, 0.5066447397662118)\n",
      "Generated model with 65 topics\n",
      "Perplexity:  -10.129821907500082\n",
      "Evaluation: (array([[143, 146],\n",
      "       [154, 157]]), 0.5, 0.49981664833149986, 0.49981641985335834, 0.4997276294871652)\n",
      "Generated model with 66 topics\n",
      "Perplexity:  -10.170196665545847\n",
      "Evaluation: (array([[140, 149],\n",
      "       [154, 157]]), 0.495, 0.49463118580765636, 0.49462610843467325, 0.49459427151240853)\n",
      "Generated model with 67 topics\n",
      "Perplexity:  -10.161861904166399\n",
      "Evaluation: (array([[137, 152],\n",
      "       [166, 145]]), 0.47, 0.47018035136847014, 0.4701431925143804, 0.4699057610241821)\n",
      "Generated model with 68 topics\n",
      "Perplexity:  -10.218853232624948\n",
      "Evaluation: (array([[179, 110],\n",
      "       [161, 150]]), 0.5483333333333333, 0.5516968325791856, 0.5508461375849754, 0.5472757191104776)\n",
      "Generated model with 69 topics\n",
      "Perplexity:  -10.290071056784068\n",
      "Evaluation: (array([[149, 140],\n",
      "       [186, 125]]), 0.45666666666666667, 0.4582371163052661, 0.45875009735310807, 0.45579594017094016)\n",
      "Generated model with 70 topics\n",
      "Perplexity:  -10.376648694292477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[131, 158],\n",
      "       [146, 165]]), 0.49333333333333335, 0.4918800505191626, 0.4919169105130231, 0.4917011291814828)\n",
      "Generated model with 71 topics\n",
      "Perplexity:  -10.398818748602292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[141, 148],\n",
      "       [148, 163]]), 0.5066666666666667, 0.5060025144917055, 0.5060025144917055, 0.5060025144917055)\n",
      "Generated model with 72 topics\n",
      "Perplexity:  -10.44653157243339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[168, 121],\n",
      "       [163, 148]]), 0.5266666666666666, 0.5288693718482912, 0.528599561632862, 0.5261401557285873)\n",
      "Generated model with 73 topics\n",
      "Perplexity:  -10.561724142290842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[114, 175],\n",
      "       [125, 186]]), 0.5, 0.49611145238122834, 0.49626720368495425, 0.4926948051948052)\n",
      "Generated model with 74 topics\n",
      "Perplexity:  -10.605586522606464\n",
      "Evaluation: (array([[148, 141],\n",
      "       [152, 159]]), 0.5116666666666667, 0.5116666666666667, 0.5116823729681015, 0.5115024772215107)\n",
      "Generated model with 75 topics\n",
      "Perplexity:  -10.678830246928456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[141, 148],\n",
      "       [156, 155]]), 0.49333333333333335, 0.49314931493149317, 0.4931407781573004, 0.4930573312136608)\n",
      "Generated model with 76 topics\n",
      "Perplexity:  -10.789658100654034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[156, 133],\n",
      "       [171, 140]]), 0.49333333333333335, 0.4949423665019995, 0.49497657962371633, 0.49297278064401356)\n",
      "Generated model with 77 topics\n",
      "Perplexity:  -10.82000454400153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[169, 120],\n",
      "       [166, 145]]), 0.5233333333333333, 0.5258237116305267, 0.5255065143136883, 0.5225694444444444)\n",
      "Generated model with 78 topics\n",
      "Perplexity:  -10.82921193024546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[148, 141],\n",
      "       [166, 145]]), 0.48833333333333334, 0.4891652933054207, 0.48917433438289254, 0.488320541346867)\n",
      "Generated model with 79 topics\n",
      "Perplexity:  -10.915220298450054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[130, 159],\n",
      "       [147, 164]]), 0.49, 0.488527008751439, 0.488579089664994, 0.48835705766293985)\n",
      "Generated model with 80 topics\n",
      "Perplexity:  -10.99682142073268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[174, 115],\n",
      "       [171, 140]]), 0.5233333333333333, 0.5266837169650469, 0.5261184481358271, 0.5217977728483686)\n",
      "Generated model with 81 topics\n",
      "Perplexity:  -11.027631000837166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[163, 126],\n",
      "       [140, 171]]), 0.5566666666666666, 0.5568556855685569, 0.5569265345631349, 0.5565878378378378)\n",
      "Generated model with 82 topics\n",
      "Perplexity:  -11.127201917789733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[162, 127],\n",
      "       [149, 162]]), 0.54, 0.5407269773807007, 0.5407269773807007, 0.54)\n",
      "Generated model with 83 topics\n",
      "Perplexity:  -11.291527597341583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[168, 121],\n",
      "       [169, 142]]), 0.5166666666666667, 0.5192201374237004, 0.518953259382058, 0.5157573666106355)\n",
      "Generated model with 84 topics\n",
      "Perplexity:  -11.419193395116016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[162, 127],\n",
      "       [174, 137]]), 0.49833333333333335, 0.5005411255411255, 0.5005340513356846, 0.4974608695652174)\n",
      "Generated model with 85 topics\n",
      "Perplexity:  -11.496999361916753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[163, 126],\n",
      "       [172, 139]]), 0.5033333333333333, 0.5055477330329484, 0.5054795892255143, 0.5025373931623931)\n",
      "Generated model with 86 topics\n",
      "Perplexity:  -11.661987467454416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[134, 155],\n",
      "       [140, 171]]), 0.5083333333333333, 0.5067954860955622, 0.5067535241825121, 0.506456497068017)\n",
      "Generated model with 87 topics\n",
      "Perplexity:  -11.751206846468662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[166, 123],\n",
      "       [167, 144]]), 0.5166666666666667, 0.5189121705975639, 0.5187084858532026, 0.5160159770357925)\n",
      "Generated model with 88 topics\n",
      "Perplexity:  -11.85946405080409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[140, 149],\n",
      "       [136, 175]]), 0.525, 0.5236849168008588, 0.5235650151870849, 0.5233781618005714)\n",
      "Generated model with 89 topics\n",
      "Perplexity:  -12.02055716966423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[126, 163],\n",
      "       [147, 164]]), 0.48333333333333334, 0.48153375676311455, 0.48165867444008054, 0.48125257979227787)\n",
      "Generated model with 90 topics\n",
      "Perplexity:  -12.15335934886508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[173, 116],\n",
      "       [167, 144]]), 0.5283333333333333, 0.5313348416289593, 0.5308192124968012, 0.5272288874843731)\n",
      "Generated model with 91 topics\n",
      "Perplexity:  -12.267225051810371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[175, 114],\n",
      "       [166, 145]]), 0.5333333333333333, 0.5365210203919881, 0.5358871371510586, 0.5321637426900585)\n",
      "Generated model with 92 topics\n",
      "Perplexity:  -12.226536797135388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[157, 132],\n",
      "       [160, 151]]), 0.5133333333333333, 0.5144185216974507, 0.5143915708897517, 0.5132846617995133)\n",
      "Generated model with 93 topics\n",
      "Perplexity:  -12.574567853827032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[163, 126],\n",
      "       [173, 138]]), 0.5016666666666667, 0.5039231601731602, 0.5038718721837137, 0.5007999999999999)\n",
      "Generated model with 94 topics\n",
      "Perplexity:  -12.7841064110396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[129, 160],\n",
      "       [156, 155]]), 0.47333333333333333, 0.47234753550543024, 0.4723795324825599, 0.4723425098240029)\n",
      "Generated model with 95 topics\n",
      "Perplexity:  -12.865022491867531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[146, 143],\n",
      "       [172, 139]]), 0.475, 0.4760136491368928, 0.4760678245196319, 0.4749285319390695)\n",
      "Generated model with 96 topics\n",
      "Perplexity:  -13.058145828699189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[154, 135],\n",
      "       [168, 143]]), 0.495, 0.49632467938692526, 0.4963395231366615, 0.49483020681951434)\n",
      "Generated model with 97 topics\n",
      "Perplexity:  -13.181767901634517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[145, 144],\n",
      "       [152, 159]]), 0.5066666666666667, 0.5064839817315065, 0.5064920615494164, 0.5063979277606697)\n",
      "Generated model with 98 topics\n",
      "Perplexity:  -13.383846318674188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: (array([[164, 125],\n",
      "       [161, 150]]), 0.5233333333333333, 0.525034965034965, 0.5248945804915498, 0.5230736734444308)\n",
      "Generated model with 99 topics\n",
      "Perplexity:  -13.532215151799685\n",
      "Evaluation: (array([[149, 140],\n",
      "       [138, 173]]), 0.5366666666666666, 0.5359397090091393, 0.535920515359539, 0.5359241452991452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifr/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_list, perplexitiy_values, evaluations = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (38,) and (35,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [219]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      5\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(start, limit, step)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mperplexitiy_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNum Topics\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPerplexity\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/matplotlib/pyplot.py:2769\u001B[0m, in \u001B[0;36mplot\u001B[0;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2767\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[1;32m   2768\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\u001B[38;5;241m*\u001B[39margs, scalex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, scaley\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 2769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2770\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2771\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/matplotlib/axes/_axes.py:1632\u001B[0m, in \u001B[0;36mAxes.plot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[1;32m   1392\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1629\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[1;32m   1630\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1631\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[0;32m-> 1632\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[1;32m   1633\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m   1634\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[0;32m~/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/matplotlib/axes/_base.py:312\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[0;34m(self, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m    310\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    311\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 312\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/uni/nlp/nlp_project/venv/lib64/python3.10/site-packages/matplotlib/axes/_base.py:498\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[0;34m(self, tup, kwargs, return_kwargs)\u001B[0m\n\u001B[1;32m    495\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes\u001B[38;5;241m.\u001B[39myaxis\u001B[38;5;241m.\u001B[39mupdate_units(y)\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 498\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y must have same first dimension, but \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    499\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave shapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y can be no greater than 2D, but have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    502\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (38,) and (35,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, perplexitiy_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.legend(\"perplexitiy_values\", loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "saving = [(model_list, \"model_list\"), (perplexitiy_values, \"perplexitiy_values\"), (evaluations, \"evaluations\")]\n",
    "\n",
    "for obj, file_name in saving:\n",
    "    file = open(file_name, \"wb\")\n",
    "    pickle.dump(obj, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}